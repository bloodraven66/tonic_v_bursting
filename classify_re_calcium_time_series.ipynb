{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(folder_count, file, signal, chunk_size, slide_size=15, norm=None):\n",
    "        data = []\n",
    "        label = []\n",
    "        f_label = []\n",
    "        n_chunks = int((len(signal)-chunk_size)/slide_size) +  1\n",
    "        for i in range(n_chunks):\n",
    "            chunk = signal[int(i*slide_size) : int(chunk_size+(i*slide_size))]\n",
    "            if norm =='inside_chunking':\n",
    "                chunk = normalise_data(chunk)\n",
    "            data.append(chunk)\n",
    "            label.append([int(file)])\n",
    "            f_label.append(folder_count)\n",
    "        return data, label, f_label, n_chunks\n",
    "    \n",
    "def normalise_data(data, norm_type='min_max'):\n",
    "    if norm_type=='z':\n",
    "        m = np.mean(data) \n",
    "        sd = np.sqrt((sum(data-m)**2)/len(data))\n",
    "        data = (data-m)/sd   \n",
    "    elif norm_type=='min_max':\n",
    "        data = (data - min(data))/(max(data)-min(data))\n",
    "    return data\n",
    "\n",
    "def load_data(fold, chunk_s, slide_s, norm=False, split_percent=0.9, dump=False, load_dump=False):\n",
    "        if load_dump:\n",
    "            with open('processed/{}_{}_ch{}_s{}.npy'.format(norm, fold, chunk_s, slide_s), 'rb') as f:\n",
    "                return np.load(f, allow_pickle=True)\n",
    "        data_train, label_train = [], []\n",
    "        data_test, label_test = [], []\n",
    "        train_f, test_f = [], []\n",
    "        chunks_count, full_label = [], []\n",
    "        train_set, test_set = [], []\n",
    "        data_val, label_val = [], []\n",
    "        folder_count = 0\n",
    "        files = sorted(os.listdir('data'))\n",
    "        f_check = {0:[], 1:[], 'l0':[], 'l1':[]}\n",
    "        \n",
    "        for i in range(len(files)):\n",
    "            file_label_ = int(files[i][-5])\n",
    "            with h5py.File('data/'+files[i], 'r') as f:\n",
    "                    data = np.array(f['reconstructed_calcium'])\n",
    "            if file_label_ == 1:\n",
    "                f_check[1].append(files[i])\n",
    "                f_check['l1'].append(len(data))\n",
    "            elif file_label_ == 0:\n",
    "                f_check[0].append(files[i])\n",
    "                f_check['l0'].append(len(data))\n",
    "\n",
    "        temp0 = list(zip(f_check['l0'], f_check[0]))\n",
    "        temp0.sort(reverse=True)\n",
    "        f_check['l0'], f_check[0] = zip(*temp0)  \n",
    "        temp0 = list(zip(f_check['l1'], f_check[1]))\n",
    "        temp0.sort(reverse=True)\n",
    "        f_check['l1'], f_check[1] = zip(*temp0)\n",
    "        f1, f2, f3, f4, f5 = [], [], [], [], []\n",
    "        \n",
    "        for i in range(len(f_check['l1'])):\n",
    "            if i%5 == 0: f1.append(f_check[1][i])\n",
    "            elif i%5 == 1: f2.append(f_check[1][i])\n",
    "            elif i%5 == 2: f3.append(f_check[1][i])\n",
    "            elif i%5 == 3: f4.append(f_check[1][i])\n",
    "            elif i%5 == 4: f5.append(f_check[1][i])\n",
    "        for i in range(len(f_check[0])):\n",
    "            if i%5 == 0: f1.append(f_check[0][i])\n",
    "            elif i%5 == 1: f2.append(f_check[0][i])\n",
    "            elif i%5 == 2: f3.append(f_check[0][i])\n",
    "            elif i%5 == 3: f4.append(f_check[0][i])\n",
    "            elif i%5 == 4: f5.append(f_check[0][i])\n",
    "                \n",
    "        if fold == '1': train_set = f2 + f3 + f4 + f5; test_set = f1;\n",
    "        if fold == '2': train_set = f1 + f3 + f4 + f5; test_set = f2;\n",
    "        if fold == '3': train_set = f1 + f2 + f4 + f5; test_set = f3;\n",
    "        if fold == '4': train_set = f1 + f2 + f3 + f5; test_set = f4;\n",
    "        if fold == '5': train_set = f1 + f2 + f3 + f4; test_set = f5;\n",
    "\n",
    "        for file in train_set:\n",
    "            with h5py.File('data/'+file, 'r') as f:\n",
    "                data = np.array(f['reconstructed_calcium'])\n",
    "            if norm=='before_chunking': data = normalise_data(data);\n",
    "            data_, label, f_label, c_c = chunks(folder_count, file=file[-5], signal=data, \n",
    "                                                chunk_size=chunk_s, slide_size=slide_s, norm=norm)\n",
    "            data_train.extend(data_); label_train.extend(label); train_f.extend(f_label)\n",
    "            folder_count += 1\n",
    "            \n",
    "        for file in test_set:\n",
    "            with h5py.File('data/'+file, 'r') as f:\n",
    "                data = np.array(f['reconstructed_calcium'])\n",
    "            if norm=='before_chunking': data = normalise_data(data);\n",
    "            data_, label, f_label, c_c = chunks(folder_count, file=file[-5], signal=data, \n",
    "                                                chunk_size=chunk_s, slide_size=slide_s, norm=norm)\n",
    "            data_test.extend(data_); chunks_count.append(c_c); label_test.extend(label)\n",
    "            test_f.extend(f_label); full_label.append(int(file[-5]))\n",
    "            folder_count += 1\n",
    "\n",
    "        data_val = data_train[int(len(data_train)*split_percent):]\n",
    "        data_train = data_train[:int(len(data_train)*split_percent)]\n",
    "        label_val = label_train[int(len(label_train)*split_percent):]\n",
    "        label_train = label_train[:int(len(label_train)*split_percent)]\n",
    "        train_f = train_f[:int(len(train_f)*split_percent)]\n",
    "        if dump==True:\n",
    "            with open('processed/{}_{}_ch{}_s{}.npy'.format(norm, fold, chunk_s, slide_s), 'wb') as f:\n",
    "                np.save(f, np.array([data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label]))\n",
    "        \n",
    "        return data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "for fold in ['1', '2', '3', '4', '5']:\n",
    "    for ss in [15, 30, 45]:\n",
    "        a = load_data(chunk_s=300, slide_s=ss, fold=fold, norm='inside_chunking', dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    }
   ],
   "source": [
    "a = \n",
    "data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feat, k, s, p, z):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, feat, kernel_size=k,stride=s)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(p)\n",
    "        self.feat = feat\n",
    "        self.z = z\n",
    "        self.linear = nn.Linear(feat*z,2)\n",
    "    def forward(self, x):\n",
    "        c_in = x.permute(0, 2, 1)a, b\n",
    "        out = self.conv(c_in)\n",
    "        out = self.relu(out)\n",
    "        c_out = self.pool(out)\n",
    "        c_out = c_out.reshape(c_out.shape[0], self.feat*self.z)\n",
    "        c_out = self.linear(c_out)\n",
    "        return c_out\n",
    "\n",
    "def train():   \n",
    "    file_acc = {}\n",
    "    correct, loss_ = 0, 0\n",
    "    model.train()\n",
    "    outs, labs = [], []\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels, f = data\n",
    "        optimizer.zero_grad()\n",
    "        if cuda_enabled==True:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        else:\n",
    "            inputs, labels = inputs, labels.squeeze()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        for j in range(len(f)):\n",
    "            file = f[j].item()\n",
    "            if file not in file_acc:\n",
    "                file_acc[file] = [0, 0]\n",
    "            if comp[j] == True:\n",
    "                file_acc[file][0]+=1\n",
    "                file_acc[file][1]+=1\n",
    "            else:\n",
    "                file_acc[file][1]+=1\n",
    "    for i in file_acc.keys():\n",
    "        file_acc[i] = [train_set[i], file_acc[i][0]/file_acc[i][1]*100]\n",
    "    return loss_/len(trainloader), correct/len(trainloader)*100, file_acc\n",
    "\n",
    "def eval_(loader, mode='test'):\n",
    "    file_acc = {}\n",
    "    correct, loss_ = 0, 0\n",
    "    model.eval()\n",
    "    outs, labs = [], []\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        if mode=='test':\n",
    "            inputs, labels, f = data\n",
    "        else:\n",
    "            inputs, labels = data\n",
    "        if cuda_enabled==True:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        else:\n",
    "            inputs, labels = inputs, labels.squeeze()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        if mode=='test':\n",
    "            for j in range(len(f)):\n",
    "                file = f[j].item()\n",
    "                if file not in file_acc:\n",
    "                    file_acc[file] = [0, 0]\n",
    "                if comp[j] == True:\n",
    "                    file_acc[file][0]+=1\n",
    "                    file_acc[file][1]+=1\n",
    "                else:\n",
    "                    file_acc[file][1]+=1\n",
    "    if mode=='test':\n",
    "        for i in file_acc.keys():\n",
    "            file_acc[i] = [test_set[i-len(train_set)], file_acc[i][0]/file_acc[i][1]*100]\n",
    "        return loss_/len(loader), correct/len(loader)*100, file_acc\n",
    "    else:\n",
    "        return loss_/len(loader), correct/len(loader)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/810, fold 1, train acc: 83.42, val acc: 95.94,  15 8 5 1 2 90 60.77\n",
      "1/810, fold 2, train acc: 77.21, val acc: 93.09,  15 8 5 1 2 18 29.46\n",
      "1/810, fold 3, train acc: 78.4, val acc: 95.49,  15 8 5 1 2 33 39.14\n",
      "1/810, fold 4, train acc: 84.11, val acc: 93.6,  15 8 5 1 2 79 44.41\n",
      "1/810, fold 5, train acc: 82.66, val acc: 84.32,  15 8 5 1 2 49 40.32\n",
      "avg:  92.49\n",
      "2/810, fold 1, train acc: 84.35, val acc: 95.31,  15 8 5 1 3 76 43.54\n",
      "2/810, fold 2, train acc: 74.71, val acc: 60.48,  15 8 5 1 3 54 40.06\n",
      "2/810, fold 3, train acc: 80.27, val acc: 66.78,  15 8 5 1 3 70 44.77\n",
      "2/810, fold 4, train acc: 75.42, val acc: 60.4,  15 8 5 1 3 23 27.66\n",
      "2/810, fold 5, train acc: 58.59, val acc: 100.0,  15 8 5 1 3 0 18.05\n",
      "avg:  76.59\n",
      "3/810, fold 1, train acc: 79.27, val acc: 58.91,  15 8 5 5 2 28 27.76\n",
      "3/810, fold 2, train acc: 78.53, val acc: 86.21,  15 8 5 5 2 28 28.48\n",
      "3/810, fold 3, train acc: 79.32, val acc: 94.75,  15 8 5 5 2 18 25.28\n",
      "3/810, fold 4, train acc: 78.2, val acc: 89.95,  15 8 5 5 2 14 23.71\n",
      "3/810, fold 5, train acc: 80.81, val acc: 76.34,  15 8 5 5 2 79 41.92\n",
      "avg:  81.23\n",
      "4/810, fold 1, train acc: 79.24, val acc: 57.58,  15 8 5 5 3 92 40.07\n",
      "4/810, fold 2, train acc: 79.06, val acc: 92.56,  15 8 5 5 3 8 20.31\n",
      "4/810, fold 3, train acc: 78.21, val acc: 61.25,  15 8 5 5 3 35 32.13\n",
      "4/810, fold 4, train acc: 81.58, val acc: 86.85,  15 8 5 5 3 19 25.33\n",
      "4/810, fold 5, train acc: 80.03, val acc: 77.69,  15 8 5 5 3 25 27.66\n",
      "avg:  75.18\n",
      "5/810, fold 1, train acc: 80.4, val acc: 76.8,  15 8 5 2 2 10 20.61\n",
      "5/810, fold 2, train acc: 77.03, val acc: 89.02,  15 8 5 2 2 23 26.23\n",
      "5/810, fold 3, train acc: 86.9, val acc: 97.77,  15 8 5 2 2 96 41.26\n",
      "5/810, fold 4, train acc: 83.05, val acc: 90.72,  15 8 5 2 2 32 31.05\n",
      "5/810, fold 5, train acc: 81.11, val acc: 81.29,  15 8 5 2 2 33 31.76\n",
      "avg:  87.12\n",
      "6/810, fold 1, train acc: 81.35, val acc: 72.58,  15 8 5 2 3 44 35.86\n",
      "6/810, fold 2, train acc: 82.17, val acc: 85.68,  15 8 5 2 3 19 25.56\n",
      "6/810, fold 3, train acc: 78.43, val acc: 87.71,  15 8 5 2 3 15 25.24\n",
      "6/810, fold 4, train acc: 79.62, val acc: 95.61,  15 8 5 2 3 19 26.46\n",
      "6/810, fold 5, train acc: 74.4, val acc: 62.39,  15 8 5 2 3 2 18.39\n",
      "avg:  80.79\n",
      "7/810, fold 1, train acc: 80.96, val acc: 60.78,  15 8 10 1 2 89 39.82\n",
      "7/810, fold 2, train acc: 78.26, val acc: 63.37,  15 8 10 1 2 64 40.9\n",
      "7/810, fold 3, train acc: 85.12, val acc: 93.72,  15 8 10 1 2 94 42.22\n",
      "7/810, fold 4, train acc: 86.37, val acc: 96.08,  15 8 10 1 2 51 40.43\n",
      "7/810, fold 5, train acc: 81.91, val acc: 81.83,  15 8 10 1 2 14 23.9\n",
      "avg:  79.16\n",
      "8/810, fold 1, train acc: 85.28, val acc: 94.53,  15 8 10 1 3 37 30.86\n",
      "8/810, fold 2, train acc: 80.47, val acc: 96.54,  15 8 10 1 3 4 19.23\n",
      "8/810, fold 3, train acc: 87.22, val acc: 95.01,  15 8 10 1 3 57 42.43\n",
      "8/810, fold 4, train acc: 86.88, val acc: 94.84,  15 8 10 1 3 42 36.61\n",
      "8/810, fold 5, train acc: 78.97, val acc: 56.35,  15 8 10 1 3 53 43.14\n",
      "avg:  87.45\n",
      "9/810, fold 1, train acc: 83.01, val acc: 73.75,  15 8 10 10 2 58 41.18\n",
      "9/810, fold 2, train acc: 80.68, val acc: 87.36,  15 8 10 10 2 34 32.36\n",
      "9/810, fold 3, train acc: 77.04, val acc: 62.18,  15 8 10 10 2 18 26.86\n",
      "9/810, fold 4, train acc: 84.02, val acc: 77.33,  15 8 10 10 2 77 42.63\n",
      "9/810, fold 5, train acc: 80.85, val acc: 86.14,  15 8 10 10 2 37 34.47\n",
      "avg:  77.35\n",
      "10/810, fold 1, train acc: 79.23, val acc: 62.11,  15 8 10 10 3 18 24.62\n",
      "10/810, fold 2, train acc: 80.16, val acc: 79.68,  15 8 10 10 3 41 35.26\n",
      "10/810, fold 3, train acc: 81.43, val acc: 95.15,  15 8 10 10 3 35 34.42\n",
      "10/810, fold 4, train acc: 80.21, val acc: 65.56,  15 8 10 10 3 87 44.18\n",
      "10/810, fold 5, train acc: 83.88, val acc: 77.09,  15 8 10 10 3 44 37.82\n",
      "avg:  75.92\n",
      "11/810, fold 1, train acc: 84.5, val acc: 88.91,  15 8 10 5 2 98 40.24\n",
      "11/810, fold 2, train acc: 74.87, val acc: 60.3,  15 8 10 5 2 45 35.57\n",
      "11/810, fold 3, train acc: 84.87, val acc: 95.49,  15 8 10 5 2 45 39.23\n",
      "11/810, fold 4, train acc: 82.91, val acc: 83.06,  15 8 10 5 2 43 37.91\n",
      "11/810, fold 5, train acc: 80.88, val acc: 80.61,  15 8 10 5 2 5 21.23\n",
      "avg:  81.67\n",
      "12/810, fold 1, train acc: 81.33, val acc: 87.19,  15 8 10 5 3 9 21.2\n",
      "12/810, fold 2, train acc: 76.9, val acc: 86.15,  15 8 10 5 3 2 19.26\n",
      "12/810, fold 3, train acc: 83.18, val acc: 84.85,  15 8 10 5 3 81 44.28\n",
      "12/810, fold 4, train acc: 84.23, val acc: 86.92,  15 8 10 5 3 58 43.43\n",
      "12/810, fold 5, train acc: 80.3, val acc: 73.19,  15 8 10 5 3 22 26.97\n",
      "avg:  83.66\n",
      "13/810, fold 1, train acc: 80.87, val acc: 64.14,  15 8 20 1 2 42 33.73\n",
      "13/810, fold 2, train acc: 78.05, val acc: 64.97,  15 8 20 1 2 24 29.31\n",
      "13/810, fold 3, train acc: 82.5, val acc: 86.77,  15 8 20 1 2 15 30.52\n",
      "13/810, fold 4, train acc: 79.61, val acc: 66.11,  15 8 20 1 2 25 45.56\n",
      "13/810, fold 5, train acc: 76.5, val acc: 54.87,  15 8 20 1 2 2 22.95\n",
      "avg:  67.37\n",
      "14/810, fold 1, train acc: 83.25, val acc: 80.47,  15 8 20 1 3 34 33.24\n",
      "14/810, fold 2, train acc: 84.75, val acc: 91.33,  15 8 20 1 3 77 48.83\n",
      "14/810, fold 3, train acc: 88.25, val acc: 95.24,  15 8 20 1 3 82 47.1\n",
      "14/810, fold 4, train acc: 82.39, val acc: 69.45,  15 8 20 1 3 43 42.14\n",
      "14/810, fold 5, train acc: 77.68, val acc: 56.16,  15 8 20 1 3 2 20.62\n",
      "avg:  78.53\n",
      "15/810, fold 1, train acc: 78.8, val acc: 59.45,  15 8 20 20 2 61 42.55\n",
      "15/810, fold 2, train acc: 72.71, val acc: 53.84,  15 8 20 20 2 18 25.72\n",
      "15/810, fold 3, train acc: 80.12, val acc: 90.16,  15 8 20 20 2 40 36.17\n",
      "15/810, fold 4, train acc: 80.94, val acc: 83.85,  15 8 20 20 2 74 42.53\n",
      "15/810, fold 5, train acc: 81.25, val acc: 77.42,  15 8 20 20 2 9 22.61\n",
      "avg:  72.94\n",
      "16/810, fold 1, train acc: 81.92, val acc: 69.38,  15 8 20 20 3 41 36.04\n",
      "16/810, fold 2, train acc: 76.59, val acc: 65.03,  15 8 20 20 3 53 39.78\n",
      "16/810, fold 3, train acc: 78.99, val acc: 63.79,  15 8 20 20 3 98 43.09\n",
      "16/810, fold 4, train acc: 79.27, val acc: 61.66,  15 8 20 20 3 34 32.43\n",
      "16/810, fold 5, train acc: 69.92, val acc: 57.71,  15 8 20 20 3 0 18.21\n",
      "avg:  63.51\n",
      "17/810, fold 1, train acc: 83.53, val acc: 82.19,  15 8 20 10 2 19 26.7\n",
      "17/810, fold 2, train acc: 82.26, val acc: 82.85,  15 8 20 10 2 99 44.15\n",
      "17/810, fold 3, train acc: 79.15, val acc: 64.31,  15 8 20 10 2 35 34.3\n",
      "17/810, fold 4, train acc: 81.12, val acc: 86.48,  15 8 20 10 2 31 32.77\n",
      "17/810, fold 5, train acc: 82.69, val acc: 77.04,  15 8 20 10 2 51 41.3\n",
      "avg:  78.57\n",
      "18/810, fold 1, train acc: 79.87, val acc: 58.13,  15 8 20 10 3 26 28.11\n",
      "18/810, fold 2, train acc: 76.29, val acc: 63.41,  15 8 20 10 3 72 41.56\n",
      "18/810, fold 3, train acc: 78.39, val acc: 64.49,  15 8 20 10 3 64 42.45\n",
      "18/810, fold 4, train acc: 84.38, val acc: 88.64,  15 8 20 10 3 68 42.4\n",
      "18/810, fold 5, train acc: 83.06, val acc: 79.25,  15 8 20 10 3 12 22.79\n",
      "avg:  70.78\n",
      "19/810, fold 1, train acc: 82.37, val acc: 83.52,  15 10 5 1 2 86 41.65\n",
      "19/810, fold 2, train acc: 76.82, val acc: 91.64,  15 10 5 1 2 11 21.66\n",
      "19/810, fold 3, train acc: 87.44, val acc: 98.81,  15 10 5 1 2 84 42.65\n",
      "19/810, fold 4, train acc: 81.71, val acc: 88.24,  15 10 5 1 2 32 31.22\n",
      "19/810, fold 5, train acc: 82.49, val acc: 87.09,  15 10 5 1 2 33 32.1\n",
      "avg:  89.86\n",
      "20/810, fold 1, train acc: 85.21, val acc: 90.78,  15 10 5 1 3 37 32.49\n",
      "20/810, fold 2, train acc: 81.88, val acc: 92.05,  15 10 5 1 3 11 21.81\n",
      "20/810, fold 3, train acc: 80.26, val acc: 97.89,  15 10 5 1 3 43 36.53\n",
      "20/810, fold 4, train acc: 85.36, val acc: 96.43,  15 10 5 1 3 83 66.83\n",
      "20/810, fold 5, train acc: 84.5, val acc: 89.42,  15 10 5 1 3 83 77.21\n",
      "avg:  93.31\n",
      "21/810, fold 1, train acc: 81.28, val acc: 76.88,  15 10 5 5 2 42 47.97\n",
      "21/810, fold 2, train acc: 77.86, val acc: 89.74,  15 10 5 5 2 64 55.86\n",
      "21/810, fold 3, train acc: 78.76, val acc: 89.46,  15 10 5 5 2 44 47.33\n",
      "21/810, fold 4, train acc: 81.7, val acc: 84.14,  15 10 5 5 2 20 29.13\n",
      "21/810, fold 5, train acc: 81.27, val acc: 76.24,  15 10 5 5 2 26 33.68\n",
      "avg:  83.29\n",
      "22/810, fold 1, train acc: 72.16, val acc: 91.33,  15 10 5 5 3 0 19.19\n",
      "22/810, fold 2, train acc: 75.65, val acc: 79.0,  15 10 5 5 3 3 21.25\n",
      "22/810, fold 3, train acc: 82.53, val acc: 95.91,  15 10 5 5 3 92 47.28\n",
      "22/810, fold 4, train acc: 80.51, val acc: 81.54,  15 10 5 5 3 27 31.85\n",
      "22/810, fold 5, train acc: 82.63, val acc: 80.93,  15 10 5 5 3 68 47.55\n",
      "avg:  85.74\n",
      "23/810, fold 1, train acc: 80.61, val acc: 68.2,  15 10 5 2 2 23 28.55\n",
      "23/810, fold 2, train acc: 76.59, val acc: 95.17,  15 10 5 2 2 10 24.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/810, fold 3, train acc: 85.35, val acc: 99.4,  15 10 5 2 2 60 46.26\n",
      "23/810, fold 4, train acc: 81.25, val acc: 89.65,  15 10 5 2 2 14 26.86\n",
      "23/810, fold 5, train acc: 80.28, val acc: 79.56,  15 10 5 2 2 14 26.33\n",
      "avg:  86.4\n",
      "24/810, fold 1, train acc: 83.05, val acc: 92.27,  15 10 5 2 3 84 45.3\n",
      "24/810, fold 2, train acc: 77.88, val acc: 94.63,  15 10 5 2 3 16 25.86\n",
      "24/810, fold 3, train acc: 80.96, val acc: 96.25,  15 10 5 2 3 10 23.96\n",
      "24/810, fold 4, train acc: 79.54, val acc: 92.77,  15 10 5 2 3 8 23.04\n",
      "24/810, fold 5, train acc: 86.13, val acc: 82.04,  15 10 5 2 3 98 48.27\n",
      "avg:  91.59\n",
      "25/810, fold 1, train acc: 84.96, val acc: 96.25,  15 10 10 1 2 46 39.32\n",
      "25/810, fold 2, train acc: 82.15, val acc: 91.83,  15 10 10 1 2 35 36.34\n",
      "25/810, fold 3, train acc: 85.74, val acc: 90.83,  15 10 10 1 2 58 48.37\n",
      "25/810, fold 4, train acc: 79.95, val acc: 88.49,  15 10 10 1 2 12 25.47\n",
      "25/810, fold 5, train acc: 85.44, val acc: 92.63,  15 10 10 1 2 69 47.3\n",
      "avg:  92.01\n",
      "26/810, fold 1, train acc: 80.3, val acc: 60.78,  15 10 10 1 3 53 42.12\n",
      "26/810, fold 2, train acc: 84.24, val acc: 93.53,  15 10 10 1 3 39 37.11\n",
      "26/810, fold 3, train acc: 84.04, val acc: 91.98,  15 10 10 1 3 23 31.16\n",
      "26/810, fold 4, train acc: 87.06, val acc: 93.5,  15 10 10 1 3 33 36.02\n",
      "26/810, fold 5, train acc: 86.01, val acc: 81.94,  15 10 10 1 3 85 47.97\n",
      "avg:  84.35\n",
      "27/810, fold 1, train acc: 82.12, val acc: 71.17,  15 10 10 10 2 33 34.18\n",
      "27/810, fold 2, train acc: 78.2, val acc: 89.47,  15 10 10 10 2 30 33.07\n",
      "27/810, fold 3, train acc: 83.44, val acc: 94.4,  15 10 10 10 2 79 46.15\n",
      "27/810, fold 4, train acc: 80.22, val acc: 81.51,  15 10 10 10 2 24 30.34\n",
      "27/810, fold 5, train acc: 78.31, val acc: 78.2,  15 10 10 10 2 1 20.67\n",
      "avg:  82.95\n",
      "28/810, fold 1, train acc: 82.67, val acc: 74.22,  15 10 10 10 3 44 38.63\n",
      "28/810, fold 2, train acc: 79.29, val acc: 85.49,  15 10 10 10 3 75 64.01\n",
      "28/810, fold 3, train acc: 83.6, val acc: 81.92,  15 10 10 10 3 39 61.78\n",
      "28/810, fold 4, train acc: 81.97, val acc: 81.91,  15 10 10 10 3 95 68.69\n",
      "28/810, fold 5, train acc: 83.1, val acc: 80.91,  15 10 10 10 3 10 39.57\n",
      "avg:  80.89\n",
      "29/810, fold 1, train acc: 82.83, val acc: 91.72,  15 10 10 5 2 33 37.18\n",
      "29/810, fold 2, train acc: 81.08, val acc: 89.53,  15 10 10 5 2 23 33.22\n",
      "29/810, fold 3, train acc: 84.39, val acc: 88.03,  15 10 10 5 2 91 50.47\n",
      "29/810, fold 4, train acc: 85.83, val acc: 87.25,  15 10 10 5 2 61 50.44\n",
      "29/810, fold 5, train acc: 83.8, val acc: 78.3,  15 10 10 5 2 57 50.08\n",
      "avg:  86.97\n",
      "30/810, fold 1, train acc: 83.1, val acc: 84.3,  15 10 10 5 3 45 41.22\n",
      "30/810, fold 2, train acc: 74.86, val acc: 60.89,  15 10 10 5 3 29 33.85\n",
      "30/810, fold 3, train acc: 81.07, val acc: 85.18,  15 10 10 5 3 18 30.07\n",
      "30/810, fold 4, train acc: 86.04, val acc: 93.25,  15 10 10 5 3 94 48.6\n",
      "30/810, fold 5, train acc: 74.79, val acc: 55.28,  15 10 10 5 3 1 21.85\n",
      "avg:  75.78\n",
      "31/810, fold 1, train acc: 79.68, val acc: 63.44,  15 10 20 1 2 7 21.52\n",
      "31/810, fold 2, train acc: 76.5, val acc: 65.48,  15 10 20 1 2 8 21.96\n",
      "31/810, fold 3, train acc: 85.79, val acc: 83.63,  15 10 20 1 2 97 45.93\n",
      "31/810, fold 4, train acc: 83.03, val acc: 89.03,  15 10 20 1 2 14 26.05\n",
      "31/810, fold 5, train acc: 78.52, val acc: 56.87,  15 10 20 1 2 12 25.4\n",
      "avg:  71.69\n",
      "32/810, fold 1, train acc: 87.49, val acc: 91.8,  15 10 20 1 3 64 43.06\n",
      "32/810, fold 2, train acc: 83.8, val acc: 95.55,  15 10 20 1 3 39 35.89\n",
      "32/810, fold 3, train acc: 84.69, val acc: 80.53,  15 10 20 1 3 88 47.82\n",
      "32/810, fold 4, train acc: 87.56, val acc: 92.7,  15 10 20 1 3 55 44.78\n",
      "32/810, fold 5, train acc: 83.77, val acc: 80.63,  15 10 20 1 3 22 29.04\n",
      "avg:  88.24\n",
      "33/810, fold 1, train acc: 78.79, val acc: 58.44,  15 10 20 20 2 24 28.05\n",
      "33/810, fold 2, train acc: 74.78, val acc: 57.88,  15 10 20 20 2 29 32.29\n",
      "33/810, fold 3, train acc: 78.05, val acc: 59.77,  15 10 20 20 2 28 33.12\n",
      "33/810, fold 4, train acc: 79.06, val acc: 60.42,  15 10 20 20 2 81 47.95\n",
      "33/810, fold 5, train acc: 82.71, val acc: 79.43,  15 10 20 20 2 33 36.1\n",
      "avg:  63.19\n",
      "34/810, fold 1, train acc: 82.81, val acc: 77.11,  15 10 20 20 3 94 44.02\n",
      "34/810, fold 2, train acc: 76.74, val acc: 61.85,  15 10 20 20 3 63 45.66\n",
      "34/810, fold 3, train acc: 79.92, val acc: 88.72,  15 10 20 20 3 7 22.94\n",
      "34/810, fold 4, train acc: 79.42, val acc: 63.93,  15 10 20 20 3 98 46.33\n",
      "34/810, fold 5, train acc: 79.89, val acc: 70.53,  15 10 20 20 3 19 28.47\n",
      "avg:  72.43\n",
      "35/810, fold 1, train acc: 83.77, val acc: 83.28,  15 10 20 10 2 63 43.51\n",
      "35/810, fold 2, train acc: 78.31, val acc: 64.49,  15 10 20 10 2 76 44.65\n",
      "35/810, fold 3, train acc: 83.55, val acc: 84.54,  15 10 20 10 2 22 32.83\n",
      "35/810, fold 4, train acc: 84.63, val acc: 84.32,  15 10 20 10 2 74 78.01\n",
      "35/810, fold 5, train acc: 80.82, val acc: 69.96,  15 10 20 10 2 91 82.01\n",
      "avg:  77.32\n",
      "36/810, fold 1, train acc: 82.58, val acc: 72.73,  15 10 20 10 3 72 69.89\n",
      "36/810, fold 2, train acc: 76.42, val acc: 62.08,  15 10 20 10 3 67 77.5\n",
      "36/810, fold 3, train acc: 83.73, val acc: 91.37,  15 10 20 10 3 52 79.33\n",
      "36/810, fold 4, train acc: 78.08, val acc: 61.12,  15 10 20 10 3 8 39.27\n",
      "36/810, fold 5, train acc: 84.59, val acc: 77.61,  15 10 20 10 3 56 81.02\n",
      "avg:  72.98\n",
      "37/810, fold 1, train acc: 80.67, val acc: 84.61,  15 12 5 1 2 9 32.14\n",
      "37/810, fold 2, train acc: 76.87, val acc: 92.28,  15 12 5 1 2 23 46.95\n",
      "37/810, fold 3, train acc: 79.73, val acc: 95.36,  15 12 5 1 2 43 61.68\n",
      "37/810, fold 4, train acc: 77.77, val acc: 91.27,  15 12 5 1 2 13 32.16\n",
      "37/810, fold 5, train acc: 80.6, val acc: 85.19,  15 12 5 1 2 21 50.22\n",
      "avg:  89.74\n",
      "38/810, fold 1, train acc: 80.08, val acc: 80.0,  15 12 5 1 3 24 50.36\n",
      "38/810, fold 2, train acc: 83.92, val acc: 97.09,  15 12 5 1 3 56 80.91\n",
      "38/810, fold 3, train acc: 79.93, val acc: 92.74,  15 12 5 1 3 2 37.73\n",
      "38/810, fold 4, train acc: 86.07, val acc: 97.37,  15 12 5 1 3 84 84.83\n",
      "38/810, fold 5, train acc: 86.53, val acc: 91.77,  15 12 5 1 3 97 90.41\n",
      "avg:  91.79\n",
      "39/810, fold 1, train acc: 80.45, val acc: 86.02,  15 12 5 5 2 8 41.25\n",
      "39/810, fold 2, train acc: 80.02, val acc: 92.58,  15 12 5 5 2 46 76.07\n",
      "39/810, fold 3, train acc: 72.15, val acc: 89.4,  15 12 5 5 2 0 34.42\n",
      "39/810, fold 4, train acc: 79.94, val acc: 92.11,  15 12 5 5 2 46 62.25\n",
      "39/810, fold 5, train acc: 81.76, val acc: 88.28,  15 12 5 5 2 34 66.3\n",
      "avg:  89.68\n",
      "40/810, fold 1, train acc: 79.32, val acc: 59.22,  15 12 5 5 3 31 61.79\n",
      "40/810, fold 2, train acc: 76.05, val acc: 60.27,  15 12 5 5 3 71 77.53\n",
      "40/810, fold 3, train acc: 72.06, val acc: 97.1,  15 12 5 5 3 0 35.06\n",
      "40/810, fold 4, train acc: 81.39, val acc: 84.07,  15 12 5 5 3 35 55.81\n",
      "40/810, fold 5, train acc: 81.94, val acc: 79.93,  15 12 5 5 3 19 46.14\n",
      "avg:  76.12\n",
      "41/810, fold 1, train acc: 81.14, val acc: 86.8,  15 12 5 2 2 17 37.64\n",
      "41/810, fold 2, train acc: 74.32, val acc: 57.51,  15 12 5 2 2 48 76.09\n",
      "41/810, fold 3, train acc: 79.59, val acc: 93.45,  15 12 5 2 2 8 43.25\n",
      "41/810, fold 4, train acc: 78.7, val acc: 92.46,  15 12 5 2 2 38 61.48\n",
      "41/810, fold 5, train acc: 78.77, val acc: 80.82,  15 12 5 2 2 1 33.6\n",
      "avg:  82.21\n",
      "42/810, fold 1, train acc: 82.21, val acc: 96.8,  15 12 5 2 3 71 69.58\n",
      "42/810, fold 2, train acc: 77.15, val acc: 94.24,  15 12 5 2 3 21 52.49\n",
      "42/810, fold 3, train acc: 85.79, val acc: 98.07,  15 12 5 2 3 88 77.08\n",
      "42/810, fold 4, train acc: 80.86, val acc: 82.21,  15 12 5 2 3 12 44.52\n",
      "42/810, fold 5, train acc: 83.47, val acc: 87.21,  15 12 5 2 3 43 71.61\n",
      "avg:  91.71\n",
      "43/810, fold 1, train acc: 79.49, val acc: 58.36,  15 12 10 1 2 6 41.04\n",
      "43/810, fold 2, train acc: 74.84, val acc: 59.07,  15 12 10 1 2 90 62.67\n",
      "43/810, fold 3, train acc: 87.78, val acc: 93.57,  15 12 10 1 2 64 83.25\n",
      "43/810, fold 4, train acc: 86.75, val acc: 95.76,  15 12 10 1 2 57 87.33\n",
      "43/810, fold 5, train acc: 75.13, val acc: 57.86,  15 12 10 1 2 4 39.72\n",
      "avg:  72.92\n",
      "44/810, fold 1, train acc: 86.66, val acc: 91.88,  15 12 10 1 3 67 83.85\n",
      "44/810, fold 2, train acc: 78.37, val acc: 91.11,  15 12 10 1 3 7 41.79\n",
      "44/810, fold 3, train acc: 84.54, val acc: 95.61,  15 12 10 1 3 33 55.75\n",
      "44/810, fold 4, train acc: 82.53, val acc: 89.9,  15 12 10 1 3 28 50.97\n",
      "44/810, fold 5, train acc: 81.55, val acc: 71.11,  15 12 10 1 3 70 84.09\n",
      "avg:  87.92\n",
      "45/810, fold 1, train acc: 84.27, val acc: 89.45,  15 12 10 10 2 48 74.3\n",
      "45/810, fold 2, train acc: 75.48, val acc: 62.86,  15 12 10 10 2 11 45.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/810, fold 3, train acc: 70.62, val acc: 99.48,  15 12 10 10 2 0 39.88\n",
      "45/810, fold 4, train acc: 81.84, val acc: 84.29,  15 12 10 10 2 44 70.54\n",
      "45/810, fold 5, train acc: 81.63, val acc: 78.18,  15 12 10 10 2 9 36.79\n",
      "avg:  82.85\n",
      "46/810, fold 1, train acc: 85.01, val acc: 90.39,  15 12 10 10 3 93 81.1\n",
      "46/810, fold 2, train acc: 76.75, val acc: 61.73,  15 12 10 10 3 19 52.86\n",
      "46/810, fold 3, train acc: 77.35, val acc: 62.99,  15 12 10 10 3 11 41.72\n",
      "46/810, fold 4, train acc: 81.32, val acc: 86.23,  15 12 10 10 3 61 89.92\n",
      "46/810, fold 5, train acc: 82.32, val acc: 81.39,  15 12 10 10 3 29 51.95\n",
      "avg:  76.55\n",
      "47/810, fold 1, train acc: 82.97, val acc: 82.19,  15 12 10 5 2 44 70.52\n",
      "47/810, fold 2, train acc: 81.98, val acc: 90.59,  15 12 10 5 2 85 71.53\n",
      "47/810, fold 3, train acc: 85.43, val acc: 95.34,  15 12 10 5 2 72 82.72\n",
      "47/810, fold 4, train acc: 85.85, val acc: 94.37,  15 12 10 5 2 29 52.73\n",
      "47/810, fold 5, train acc: 82.39, val acc: 83.25,  15 12 10 5 2 44 69.48\n",
      "avg:  89.15\n",
      "48/810, fold 1, train acc: 83.44, val acc: 81.41,  15 12 10 5 3 90 72.44\n",
      "48/810, fold 2, train acc: 78.04, val acc: 65.64,  15 12 10 5 3 33 61.94\n",
      "48/810, fold 3, train acc: 86.21, val acc: 93.35,  15 12 10 5 3 86 87.14\n",
      "48/810, fold 4, train acc: 86.26, val acc: 93.57,  15 12 10 5 3 41 75.79\n",
      "48/810, fold 5, train acc: 82.19, val acc: 80.74,  15 12 10 5 3 6 39.37\n",
      "avg:  82.94\n",
      "49/810, fold 1, train acc: 81.94, val acc: 80.94,  15 12 20 1 2 40 67.29\n",
      "49/810, fold 2, train acc: 82.97, val acc: 94.16,  15 12 20 1 2 79 80.56\n",
      "49/810, fold 3, train acc: 85.62, val acc: 86.07,  15 12 20 1 2 94 84.08\n",
      "49/810, fold 4, train acc: 85.39, val acc: 87.74,  15 12 20 1 2 41 68.3\n",
      "49/810, fold 5, train acc: 83.51, val acc: 82.61,  15 12 20 1 2 7 41.99\n",
      "avg:  86.3\n",
      "50/810, fold 1, train acc: 85.44, val acc: 80.62,  15 12 20 1 3 74 75.9\n",
      "50/810, fold 2, train acc: 80.85, val acc: 73.96,  15 12 20 1 3 80 80.0\n",
      "50/810, fold 3, train acc: 85.32, val acc: 82.12,  15 12 20 1 3 47 73.53\n",
      "50/810, fold 4, train acc: 82.72, val acc: 88.36,  15 12 20 1 3 13 49.28\n",
      "50/810, fold 5, train acc: 83.66, val acc: 80.28,  15 12 20 1 3 6 42.99\n",
      "avg:  81.07\n",
      "51/810, fold 1, train acc: 80.85, val acc: 75.08,  15 12 20 20 2 48 68.21\n",
      "51/810, fold 2, train acc: 81.55, val acc: 87.24,  15 12 20 20 2 60 83.51\n",
      "51/810, fold 3, train acc: 78.02, val acc: 66.56,  15 12 20 20 2 59 72.54\n",
      "51/810, fold 4, train acc: 83.53, val acc: 91.54,  15 12 20 20 2 36 62.65\n",
      "51/810, fold 5, train acc: 82.33, val acc: 76.04,  15 12 20 20 2 49 78.53\n",
      "avg:  79.29\n",
      "52/810, fold 1, train acc: 79.97, val acc: 62.11,  15 12 20 20 3 63 70.02\n",
      "52/810, fold 2, train acc: 76.96, val acc: 63.24,  15 12 20 20 3 92 79.71\n",
      "52/810, fold 3, train acc: 78.13, val acc: 65.05,  15 12 20 20 3 79 89.91\n",
      "52/810, fold 4, train acc: 79.55, val acc: 61.32,  15 12 20 20 3 31 64.84\n",
      "52/810, fold 5, train acc: 85.45, val acc: 82.25,  15 12 20 20 3 77 69.82\n",
      "avg:  66.79\n",
      "53/810, fold 1, train acc: 81.77, val acc: 75.78,  15 12 20 10 2 7 38.55\n",
      "53/810, fold 2, train acc: 78.52, val acc: 69.82,  15 12 20 10 2 48 78.57\n",
      "53/810, fold 3, train acc: 85.73, val acc: 87.29,  15 12 20 10 2 25 54.46\n",
      "53/810, fold 4, train acc: 74.04, val acc: 85.55,  15 12 20 10 2 0 33.57\n",
      "53/810, fold 5, train acc: 76.95, val acc: 56.12,  15 12 20 10 2 3 36.45\n",
      "avg:  74.91\n",
      "54/810, fold 1, train acc: 80.6, val acc: 80.86,  15 12 20 10 3 5 34.22\n",
      "54/810, fold 2, train acc: 82.11, val acc: 88.45,  15 12 20 10 3 26 51.73\n",
      "54/810, fold 3, train acc: 77.65, val acc: 63.86,  15 12 20 10 3 5 33.25\n",
      "54/810, fold 4, train acc: 81.55, val acc: 83.87,  15 12 20 10 3 23 49.76\n",
      "54/810, fold 5, train acc: 78.73, val acc: 60.01,  15 12 20 10 3 10 43.82\n",
      "avg:  75.41\n",
      "55/810, fold 1, train acc: 80.69, val acc: 74.17,  30 8 5 1 2 6 19.35\n",
      "55/810, fold 2, train acc: 74.22, val acc: 82.9,  30 8 5 1 2 2 17.78\n",
      "55/810, fold 3, train acc: 82.38, val acc: 87.4,  30 8 5 1 2 23 18.82\n",
      "55/810, fold 4, train acc: 79.46, val acc: 87.06,  30 8 5 1 2 10 15.3\n",
      "55/810, fold 5, train acc: 80.13, val acc: 83.29,  30 8 5 1 2 15 25.87\n",
      "avg:  82.96\n",
      "56/810, fold 1, train acc: 79.88, val acc: 55.06,  30 8 5 1 3 8 21.73\n",
      "56/810, fold 2, train acc: 77.55, val acc: 87.77,  30 8 5 1 3 25 29.23\n",
      "56/810, fold 3, train acc: 76.96, val acc: 92.51,  30 8 5 1 3 7 15.13\n",
      "56/810, fold 4, train acc: 84.38, val acc: 85.75,  30 8 5 1 3 46 35.63\n",
      "56/810, fold 5, train acc: 79.99, val acc: 78.26,  30 8 5 1 3 21 29.14\n",
      "avg:  79.87\n",
      "57/810, fold 1, train acc: 81.72, val acc: 73.69,  30 8 5 5 2 68 42.27\n",
      "57/810, fold 2, train acc: 77.16, val acc: 79.77,  30 8 5 5 2 11 18.81\n",
      "57/810, fold 3, train acc: 74.64, val acc: 88.53,  30 8 5 5 2 1 18.81\n",
      "57/810, fold 4, train acc: 78.2, val acc: 76.61,  30 8 5 5 2 26 29.9\n",
      "57/810, fold 5, train acc: 80.88, val acc: 78.4,  30 8 5 5 2 42 35.99\n",
      "avg:  79.4\n",
      "58/810, fold 1, train acc: 81.49, val acc: 73.29,  30 8 5 5 3 43 22.11\n",
      "58/810, fold 2, train acc: 78.49, val acc: 81.98,  30 8 5 5 3 40 28.31\n",
      "58/810, fold 3, train acc: 80.08, val acc: 84.22,  30 8 5 5 3 64 40.89\n",
      "58/810, fold 4, train acc: 81.71, val acc: 87.34,  30 8 5 5 3 38 30.02\n",
      "58/810, fold 5, train acc: 79.47, val acc: 85.22,  30 8 5 5 3 50 37.09\n",
      "avg:  82.41\n",
      "59/810, fold 1, train acc: 79.36, val acc: 53.59,  30 8 5 2 2 97 43.33\n",
      "59/810, fold 2, train acc: 77.41, val acc: 84.0,  30 8 5 2 2 28 29.19\n",
      "59/810, fold 3, train acc: 62.41, val acc: 58.48,  30 8 5 2 2 0 17.89\n",
      "59/810, fold 4, train acc: 77.09, val acc: 87.7,  30 8 5 2 2 12 15.14\n",
      "59/810, fold 5, train acc: 80.6, val acc: 81.27,  30 8 5 2 2 17 26.53\n",
      "avg:  73.01\n",
      "60/810, fold 1, train acc: 80.09, val acc: 70.73,  30 8 5 2 3 29 25.42\n",
      "60/810, fold 2, train acc: 83.75, val acc: 88.26,  30 8 5 2 3 96 37.95\n",
      "60/810, fold 3, train acc: 82.15, val acc: 90.04,  30 8 5 2 3 21 26.86\n",
      "60/810, fold 4, train acc: 80.0, val acc: 89.35,  30 8 5 2 3 29 29.87\n",
      "60/810, fold 5, train acc: 79.37, val acc: 81.87,  30 8 5 2 3 19 24.99\n",
      "avg:  84.05\n",
      "61/810, fold 1, train acc: 85.13, val acc: 79.47,  30 8 10 1 2 76 41.46\n",
      "61/810, fold 2, train acc: 75.2, val acc: 51.73,  30 8 10 1 2 8 14.43\n",
      "61/810, fold 3, train acc: 78.06, val acc: 88.05,  30 8 10 1 2 3 15.82\n",
      "61/810, fold 4, train acc: 79.94, val acc: 89.98,  30 8 10 1 2 5 20.54\n",
      "61/810, fold 5, train acc: 79.45, val acc: 88.66,  30 8 10 1 2 5 21.97\n",
      "avg:  79.58\n",
      "62/810, fold 1, train acc: 84.06, val acc: 78.99,  30 8 10 1 3 73 37.42\n",
      "62/810, fold 2, train acc: 78.57, val acc: 78.35,  30 8 10 1 3 9 15.31\n",
      "62/810, fold 3, train acc: 78.84, val acc: 91.46,  30 8 10 1 3 15 25.43\n",
      "62/810, fold 4, train acc: 86.21, val acc: 96.16,  30 8 10 1 3 82 44.25\n",
      "62/810, fold 5, train acc: 76.7, val acc: 59.31,  30 8 10 1 3 20 28.75\n",
      "avg:  80.86\n",
      "63/810, fold 1, train acc: 81.93, val acc: 74.34,  30 8 10 10 2 58 43.75\n",
      "63/810, fold 2, train acc: 57.38, val acc: 97.49,  30 8 10 10 2 0 18.53\n",
      "63/810, fold 3, train acc: 81.2, val acc: 94.32,  30 8 10 10 2 56 45.93\n",
      "63/810, fold 4, train acc: 79.67, val acc: 81.63,  30 8 10 10 2 15 26.42\n",
      "63/810, fold 5, train acc: 82.34, val acc: 78.8,  30 8 10 10 2 56 41.58\n",
      "avg:  85.32\n",
      "64/810, fold 1, train acc: 79.98, val acc: 70.67,  30 8 10 10 3 16 24.64\n",
      "64/810, fold 2, train acc: 58.81, val acc: 66.45,  30 8 10 10 3 0 18.22\n",
      "64/810, fold 3, train acc: 80.92, val acc: 81.29,  30 8 10 10 3 21 24.62\n",
      "64/810, fold 4, train acc: 82.16, val acc: 91.19,  30 8 10 10 3 40 35.5\n",
      "64/810, fold 5, train acc: 75.94, val acc: 59.05,  30 8 10 10 3 3 17.86\n",
      "avg:  73.73\n",
      "65/810, fold 1, train acc: 83.18, val acc: 80.35,  30 8 10 5 2 96 40.65\n",
      "65/810, fold 2, train acc: 76.31, val acc: 93.88,  30 8 10 5 2 2 19.69\n",
      "65/810, fold 3, train acc: 83.33, val acc: 90.62,  30 8 10 5 2 46 38.98\n",
      "65/810, fold 4, train acc: 87.58, val acc: 92.08,  30 8 10 5 2 83 42.43\n",
      "65/810, fold 5, train acc: 81.2, val acc: 81.07,  30 8 10 5 2 70 47.14\n",
      "avg:  87.6\n",
      "66/810, fold 1, train acc: 84.07, val acc: 91.38,  30 8 10 5 3 39 34.15\n",
      "66/810, fold 2, train acc: 81.8, val acc: 94.36,  30 8 10 5 3 68 39.66\n",
      "66/810, fold 3, train acc: 79.27, val acc: 82.75,  30 8 10 5 3 6 20.46\n",
      "66/810, fold 4, train acc: 80.9, val acc: 94.52,  30 8 10 5 3 6 19.5\n",
      "66/810, fold 5, train acc: 76.45, val acc: 60.76,  30 8 10 5 3 8 19.99\n",
      "avg:  84.75\n",
      "67/810, fold 1, train acc: 83.05, val acc: 62.98,  30 8 20 1 2 71 38.69\n",
      "67/810, fold 2, train acc: 76.89, val acc: 57.66,  30 8 20 1 2 86 39.41\n",
      "67/810, fold 3, train acc: 84.06, val acc: 91.6,  30 8 20 1 2 14 25.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/810, fold 4, train acc: 80.43, val acc: 88.1,  30 8 20 1 2 17 25.43\n",
      "67/810, fold 5, train acc: 78.36, val acc: 63.51,  30 8 20 1 2 14 23.63\n",
      "avg:  72.77\n",
      "68/810, fold 1, train acc: 82.79, val acc: 77.18,  30 8 20 1 3 14 22.28\n",
      "68/810, fold 2, train acc: 78.74, val acc: 91.69,  30 8 20 1 3 1 17.59\n",
      "68/810, fold 3, train acc: 86.77, val acc: 79.28,  30 8 20 1 3 89 41.74\n",
      "68/810, fold 4, train acc: 85.13, val acc: 75.36,  30 8 20 1 3 73 45.48\n",
      "68/810, fold 5, train acc: 85.0, val acc: 88.97,  30 8 20 1 3 19 27.23\n",
      "avg:  82.5\n",
      "69/810, fold 1, train acc: 79.99, val acc: 59.29,  30 8 20 20 2 95 40.11\n",
      "69/810, fold 2, train acc: 78.42, val acc: 85.75,  30 8 20 20 2 24 27.86\n",
      "69/810, fold 3, train acc: 82.38, val acc: 86.17,  30 8 20 20 2 20 24.67\n",
      "69/810, fold 4, train acc: 83.24, val acc: 86.17,  30 8 20 20 2 71 41.3\n",
      "69/810, fold 5, train acc: 75.87, val acc: 58.42,  30 8 20 20 2 4 20.26\n",
      "avg:  75.16\n",
      "70/810, fold 1, train acc: 81.29, val acc: 71.54,  30 8 20 20 3 51 38.71\n",
      "70/810, fold 2, train acc: 81.7, val acc: 84.82,  30 8 20 20 3 90 41.03\n",
      "70/810, fold 3, train acc: 80.86, val acc: 86.26,  30 8 20 20 3 23 28.49\n",
      "70/810, fold 4, train acc: 58.62, val acc: 100.0,  30 8 20 20 3 0 15.2\n",
      "70/810, fold 5, train acc: 82.8, val acc: 75.7,  30 8 20 20 3 44 31.06\n",
      "avg:  83.66\n",
      "71/810, fold 1, train acc: 83.96, val acc: 84.64,  30 8 20 10 2 68 27.19\n",
      "71/810, fold 2, train acc: 81.96, val acc: 80.11,  30 8 20 10 2 63 43.18\n",
      "71/810, fold 3, train acc: 83.5, val acc: 81.48,  30 8 20 10 2 54 39.84\n",
      "71/810, fold 4, train acc: 83.87, val acc: 82.23,  30 8 20 10 2 31 31.41\n",
      "71/810, fold 5, train acc: 86.01, val acc: 76.9,  30 8 20 10 2 93 44.03\n",
      "avg:  81.07\n",
      "72/810, fold 1, train acc: 83.19, val acc: 75.47,  30 8 20 10 3 65 37.24\n",
      "72/810, fold 2, train acc: 81.94, val acc: 90.45,  30 8 20 10 3 81 38.67\n",
      "72/810, fold 3, train acc: 82.91, val acc: 79.81,  30 8 20 10 3 39 33.13\n",
      "72/810, fold 4, train acc: 81.44, val acc: 75.1,  30 8 20 10 3 83 43.84\n",
      "72/810, fold 5, train acc: 65.36, val acc: 59.05,  30 8 20 10 3 0 18.71\n",
      "avg:  75.98\n",
      "73/810, fold 1, train acc: 81.07, val acc: 75.04,  30 10 5 1 2 36 33.25\n",
      "73/810, fold 2, train acc: 77.41, val acc: 81.66,  30 10 5 1 2 10 24.22\n",
      "73/810, fold 3, train acc: 84.75, val acc: 89.49,  30 10 5 1 2 85 39.21\n",
      "73/810, fold 4, train acc: 76.86, val acc: 91.37,  30 10 5 1 2 5 19.75\n",
      "73/810, fold 5, train acc: 80.48, val acc: 83.83,  30 10 5 1 2 12 23.62\n",
      "avg:  84.28\n",
      "74/810, fold 1, train acc: 81.19, val acc: 72.54,  30 10 5 1 3 18 25.5\n",
      "74/810, fold 2, train acc: 56.01, val acc: 97.02,  30 10 5 1 3 0 19.46\n",
      "74/810, fold 3, train acc: 76.99, val acc: 90.24,  30 10 5 1 3 6 22.71\n",
      "74/810, fold 4, train acc: 79.13, val acc: 88.59,  30 10 5 1 3 3 12.32\n",
      "74/810, fold 5, train acc: 83.6, val acc: 85.85,  30 10 5 1 3 47 26.16\n",
      "avg:  86.85\n",
      "75/810, fold 1, train acc: 79.82, val acc: 82.26,  30 10 5 5 2 6 13.05\n",
      "75/810, fold 2, train acc: 74.03, val acc: 92.32,  30 10 5 5 2 1 11.57\n",
      "75/810, fold 3, train acc: 78.32, val acc: 94.41,  30 10 5 5 2 6 12.37\n",
      "75/810, fold 4, train acc: 77.99, val acc: 77.96,  30 10 5 5 2 49 21.52\n",
      "75/810, fold 5, train acc: 80.8, val acc: 87.41,  30 10 5 5 2 53 23.16\n",
      "avg:  86.87\n",
      "76/810, fold 1, train acc: 80.82, val acc: 73.4,  30 10 5 5 3 17 14.22\n",
      "76/810, fold 2, train acc: 77.62, val acc: 82.29,  30 10 5 5 3 12 14.14\n",
      "76/810, fold 3, train acc: 79.97, val acc: 94.55,  30 10 5 5 3 8 14.27\n",
      "76/810, fold 4, train acc: 79.42, val acc: 81.47,  30 10 5 5 3 24 18.7\n",
      "76/810, fold 5, train acc: 80.2, val acc: 77.89,  30 10 5 5 3 67 31.48\n",
      "avg:  81.92\n",
      "77/810, fold 1, train acc: 84.27, val acc: 85.02,  30 10 5 2 2 76 32.29\n",
      "77/810, fold 2, train acc: 75.66, val acc: 87.45,  30 10 5 2 2 8 15.32\n",
      "77/810, fold 3, train acc: 81.59, val acc: 83.74,  30 10 5 2 2 11 16.49\n",
      "77/810, fold 4, train acc: 82.43, val acc: 83.62,  30 10 5 2 2 34 20.62\n",
      "77/810, fold 5, train acc: 78.28, val acc: 80.05,  30 10 5 2 2 4 12.59\n",
      "avg:  83.98\n",
      "78/810, fold 1, train acc: 81.34, val acc: 76.2,  30 10 5 2 3 33 20.18\n",
      "78/810, fold 2, train acc: 80.17, val acc: 84.49,  30 10 5 2 3 20 21.75\n",
      "78/810, fold 3, train acc: 76.4, val acc: 88.58,  30 10 5 2 3 4 15.4\n",
      "78/810, fold 4, train acc: 83.85, val acc: 91.62,  30 10 5 2 3 31 24.67\n",
      "78/810, fold 5, train acc: 79.79, val acc: 84.46,  30 10 5 2 3 4 15.11\n",
      "avg:  85.07\n",
      "79/810, fold 1, train acc: 85.8, val acc: 76.51,  30 10 10 1 2 96 30.33\n",
      "79/810, fold 2, train acc: 78.07, val acc: 61.27,  30 10 10 1 2 25 22.78\n",
      "79/810, fold 3, train acc: 83.2, val acc: 84.55,  30 10 10 1 2 17 19.35\n",
      "79/810, fold 4, train acc: 82.09, val acc: 83.36,  30 10 10 1 2 36 22.02\n",
      "79/810, fold 5, train acc: 80.83, val acc: 82.87,  30 10 10 1 2 10 15.26\n",
      "avg:  77.71\n",
      "80/810, fold 1, train acc: 81.1, val acc: 82.56,  30 10 10 1 3 14 14.86\n",
      "80/810, fold 2, train acc: 81.85, val acc: 82.45,  30 10 10 1 3 28 21.35\n",
      "80/810, fold 3, train acc: 87.97, val acc: 93.36,  30 10 10 1 3 95 30.49\n",
      "80/810, fold 4, train acc: 82.64, val acc: 69.79,  30 10 10 1 3 63 28.76\n",
      "80/810, fold 5, train acc: 80.38, val acc: 83.09,  30 10 10 1 3 22 20.3\n",
      "avg:  82.25\n",
      "81/810, fold 1, train acc: 77.42, val acc: 88.65,  30 10 10 10 2 1 12.98\n",
      "81/810, fold 2, train acc: 77.22, val acc: 83.39,  30 10 10 10 2 16 15.77\n",
      "81/810, fold 3, train acc: 80.7, val acc: 95.12,  30 10 10 10 2 11 14.19\n",
      "81/810, fold 4, train acc: 82.57, val acc: 90.15,  30 10 10 10 2 21 24.87\n",
      "81/810, fold 5, train acc: 79.97, val acc: 75.08,  30 10 10 10 2 9 18.77\n",
      "avg:  86.48\n",
      "82/810, fold 1, train acc: 81.6, val acc: 80.42,  30 10 10 10 3 96 33.7\n",
      "82/810, fold 2, train acc: 78.63, val acc: 73.05,  30 10 10 10 3 20 18.3\n",
      "82/810, fold 3, train acc: 58.45, val acc: 98.01,  30 10 10 10 3 0 11.88\n",
      "82/810, fold 4, train acc: 81.98, val acc: 93.21,  30 10 10 10 3 31 21.29\n",
      "82/810, fold 5, train acc: 79.33, val acc: 68.77,  30 10 10 10 3 10 15.58\n",
      "avg:  82.69\n",
      "83/810, fold 1, train acc: 83.13, val acc: 81.2,  30 10 10 5 2 67 26.1\n",
      "83/810, fold 2, train acc: 84.22, val acc: 91.85,  30 10 10 5 2 87 29.22\n",
      "83/810, fold 3, train acc: 80.1, val acc: 88.14,  30 10 10 5 2 5 15.28\n",
      "83/810, fold 4, train acc: 80.5, val acc: 86.82,  30 10 10 5 2 15 16.34\n",
      "83/810, fold 5, train acc: 68.03, val acc: 67.75,  30 10 10 5 2 0 14.09\n",
      "avg:  83.15\n",
      "84/810, fold 1, train acc: 77.47, val acc: 60.66,  30 10 10 5 3 1 10.81\n",
      "84/810, fold 2, train acc: 75.47, val acc: 56.42,  30 10 10 5 3 32 20.57\n",
      "84/810, fold 3, train acc: 77.94, val acc: 58.74,  30 10 10 5 3 23 23.11\n",
      "84/810, fold 4, train acc: 81.13, val acc: 86.46,  30 10 10 5 3 33 28.85\n",
      "84/810, fold 5, train acc: 79.05, val acc: 82.72,  30 10 10 5 3 20 24.61\n",
      "avg:  69.0\n",
      "85/810, fold 1, train acc: 81.4, val acc: 63.77,  30 10 20 1 2 39 29.33\n",
      "85/810, fold 2, train acc: 83.82, val acc: 80.08,  30 10 20 1 2 35 27.93\n",
      "85/810, fold 3, train acc: 85.36, val acc: 86.4,  30 10 20 1 2 19 18.32\n",
      "85/810, fold 4, train acc: 83.48, val acc: 69.93,  30 10 20 1 2 90 36.32\n",
      "85/810, fold 5, train acc: 84.76, val acc: 63.79,  30 10 20 1 2 48 29.67\n",
      "avg:  72.8\n",
      "86/810, fold 1, train acc: 84.44, val acc: 74.34,  30 10 20 1 3 61 29.69\n",
      "86/810, fold 2, train acc: 81.54, val acc: 67.08,  30 10 20 1 3 62 32.69\n",
      "86/810, fold 3, train acc: 84.55, val acc: 94.03,  30 10 20 1 3 93 32.84\n",
      "86/810, fold 4, train acc: 83.99, val acc: 71.21,  30 10 20 1 3 70 32.96\n",
      "86/810, fold 5, train acc: 87.24, val acc: 86.07,  30 10 20 1 3 46 30.72\n",
      "avg:  78.55\n",
      "87/810, fold 1, train acc: 81.12, val acc: 77.21,  30 10 20 20 2 30 20.86\n",
      "87/810, fold 2, train acc: 75.71, val acc: 60.52,  30 10 20 20 2 38 22.81\n",
      "87/810, fold 3, train acc: 79.16, val acc: 87.82,  30 10 20 20 2 4 11.66\n",
      "87/810, fold 4, train acc: 80.04, val acc: 83.33,  30 10 20 20 2 7 12.67\n",
      "87/810, fold 5, train acc: 79.92, val acc: 77.38,  30 10 20 20 2 5 11.9\n",
      "avg:  77.25\n",
      "88/810, fold 1, train acc: 82.72, val acc: 82.32,  30 10 20 20 3 14 15.12\n",
      "88/810, fold 2, train acc: 80.71, val acc: 83.23,  30 10 20 20 3 36 21.16\n",
      "88/810, fold 3, train acc: 82.12, val acc: 85.17,  30 10 20 20 3 10 14.99\n",
      "88/810, fold 4, train acc: 79.73, val acc: 62.56,  30 10 20 20 3 72 26.43\n",
      "88/810, fold 5, train acc: 78.14, val acc: 60.5,  30 10 20 20 3 45 22.87\n",
      "avg:  74.76\n",
      "89/810, fold 1, train acc: 83.67, val acc: 83.31,  30 10 20 10 2 64 28.05\n",
      "89/810, fold 2, train acc: 66.72, val acc: 58.33,  30 10 20 10 2 0 16.0\n",
      "89/810, fold 3, train acc: 81.52, val acc: 90.75,  30 10 20 10 2 81 38.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/810, fold 4, train acc: 83.97, val acc: 73.48,  30 10 20 10 2 94 35.92\n",
      "89/810, fold 5, train acc: 75.97, val acc: 60.13,  30 10 20 10 2 2 17.34\n",
      "avg:  73.2\n",
      "90/810, fold 1, train acc: 83.23, val acc: 80.79,  30 10 20 10 3 65 28.82\n",
      "90/810, fold 2, train acc: 82.06, val acc: 80.38,  30 10 20 10 3 56 31.11\n",
      "90/810, fold 3, train acc: 82.48, val acc: 83.28,  30 10 20 10 3 23 23.72\n",
      "90/810, fold 4, train acc: 83.7, val acc: 76.26,  30 10 20 10 3 86 31.28\n",
      "90/810, fold 5, train acc: 84.0, val acc: 72.55,  30 10 20 10 3 48 32.36\n",
      "avg:  78.65\n",
      "91/810, fold 1, train acc: 83.61, val acc: 83.09,  30 12 5 1 2 73 28.8\n",
      "91/810, fold 2, train acc: 76.71, val acc: 85.29,  30 12 5 1 2 6 17.82\n",
      "91/810, fold 3, train acc: 78.17, val acc: 92.94,  30 12 5 1 2 39 28.73\n",
      "91/810, fold 4, train acc: 80.82, val acc: 84.47,  30 12 5 1 2 21 22.66\n",
      "91/810, fold 5, train acc: 68.88, val acc: 84.26,  30 12 5 1 2 0 13.57\n",
      "avg:  86.01\n",
      "92/810, fold 1, train acc: 82.19, val acc: 88.5,  30 12 5 1 3 89 26.73\n",
      "92/810, fold 2, train acc: 79.28, val acc: 93.9,  30 12 5 1 3 7 11.59\n",
      "92/810, fold 3, train acc: 78.64, val acc: 91.42,  30 12 5 1 3 46 26.67\n",
      "92/810, fold 4, train acc: 82.12, val acc: 90.37,  30 12 5 1 3 58 37.38\n",
      "92/810, fold 5, train acc: 78.42, val acc: 87.73,  30 12 5 1 3 1 12.85\n",
      "avg:  90.38\n",
      "93/810, fold 1, train acc: 81.93, val acc: 74.57,  30 12 5 5 2 94 25.88\n",
      "93/810, fold 2, train acc: 76.15, val acc: 83.38,  30 12 5 5 2 5 11.74\n",
      "93/810, fold 3, train acc: 79.52, val acc: 89.53,  30 12 5 5 2 27 20.08\n",
      "93/810, fold 4, train acc: 79.88, val acc: 83.58,  30 12 5 5 2 46 27.45\n",
      "93/810, fold 5, train acc: 80.88, val acc: 83.94,  30 12 5 5 2 20 15.58\n",
      "avg:  83.0\n",
      "94/810, fold 1, train acc: 81.31, val acc: 75.06,  30 12 5 5 3 65 32.31\n",
      "94/810, fold 2, train acc: 78.42, val acc: 83.87,  30 12 5 5 3 45 25.31\n",
      "94/810, fold 3, train acc: 77.6, val acc: 84.02,  30 12 5 5 3 8 13.58\n",
      "94/810, fold 4, train acc: 80.72, val acc: 88.62,  30 12 5 5 3 73 30.28\n",
      "94/810, fold 5, train acc: 81.17, val acc: 84.43,  30 12 5 5 3 8 14.27\n",
      "avg:  83.2\n",
      "95/810, fold 1, train acc: 81.26, val acc: 73.65,  30 12 5 2 2 32 20.55\n",
      "95/810, fold 2, train acc: 76.06, val acc: 84.64,  30 12 5 2 2 8 15.42\n",
      "95/810, fold 3, train acc: 74.83, val acc: 95.74,  30 12 5 2 2 2 13.43\n",
      "95/810, fold 4, train acc: 82.67, val acc: 82.19,  30 12 5 2 2 42 21.12\n",
      "95/810, fold 5, train acc: 80.39, val acc: 84.79,  30 12 5 2 2 16 15.68\n",
      "avg:  84.2\n",
      "96/810, fold 1, train acc: 80.32, val acc: 83.94,  30 12 5 2 3 17 14.71\n",
      "96/810, fold 2, train acc: 76.85, val acc: 80.25,  30 12 5 2 3 23 16.0\n",
      "96/810, fold 3, train acc: 79.87, val acc: 88.11,  30 12 5 2 3 55 25.86\n",
      "96/810, fold 4, train acc: 78.02, val acc: 86.96,  30 12 5 2 3 9 14.51\n",
      "96/810, fold 5, train acc: 70.53, val acc: 87.7,  30 12 5 2 3 0 12.93\n",
      "avg:  85.39\n",
      "97/810, fold 1, train acc: 83.76, val acc: 78.02,  30 12 10 1 2 83 31.83\n",
      "97/810, fold 2, train acc: 85.23, val acc: 82.6,  30 12 10 1 2 67 30.11\n",
      "97/810, fold 3, train acc: 80.97, val acc: 95.45,  30 12 10 1 2 8 15.6\n",
      "97/810, fold 4, train acc: 85.04, val acc: 82.76,  30 12 10 1 2 49 26.83\n",
      "97/810, fold 5, train acc: 80.02, val acc: 83.01,  30 12 10 1 2 7 13.53\n",
      "avg:  84.37\n",
      "98/810, fold 1, train acc: 84.71, val acc: 78.55,  30 12 10 1 3 97 29.08\n",
      "98/810, fold 2, train acc: 84.89, val acc: 89.35,  30 12 10 1 3 28 21.75\n",
      "98/810, fold 3, train acc: 86.22, val acc: 93.93,  30 12 10 1 3 32 21.31\n",
      "98/810, fold 4, train acc: 82.5, val acc: 91.51,  30 12 10 1 3 15 16.25\n",
      "98/810, fold 5, train acc: 78.83, val acc: 59.59,  30 12 10 1 3 13 16.46\n",
      "avg:  82.58\n",
      "99/810, fold 1, train acc: 81.72, val acc: 76.32,  30 12 10 10 2 67 31.85\n",
      "99/810, fold 2, train acc: 81.05, val acc: 93.39,  30 12 10 10 2 29 23.78\n",
      "99/810, fold 3, train acc: 80.96, val acc: 88.67,  30 12 10 10 2 28 21.2\n",
      "99/810, fold 4, train acc: 82.58, val acc: 93.84,  30 12 10 10 2 45 22.28\n",
      "99/810, fold 5, train acc: 81.01, val acc: 70.05,  30 12 10 10 2 31 18.29\n",
      "avg:  84.46\n",
      "100/810, fold 1, train acc: 80.15, val acc: 60.41,  30 12 10 10 3 33 23.73\n",
      "100/810, fold 2, train acc: 78.04, val acc: 91.39,  30 12 10 10 3 8 15.01\n",
      "100/810, fold 3, train acc: 82.14, val acc: 92.99,  30 12 10 10 3 7 13.79\n",
      "100/810, fold 4, train acc: 81.67, val acc: 91.37,  30 12 10 10 3 39 22.13\n",
      "100/810, fold 5, train acc: 81.68, val acc: 80.53,  30 12 10 10 3 12 14.24\n",
      "avg:  83.34\n",
      "101/810, fold 1, train acc: 86.46, val acc: 90.13,  30 12 10 5 2 94 33.08\n",
      "101/810, fold 2, train acc: 79.19, val acc: 87.33,  30 12 10 5 2 18 25.1\n",
      "101/810, fold 3, train acc: 77.91, val acc: 95.87,  30 12 10 5 2 1 16.31\n",
      "101/810, fold 4, train acc: 83.43, val acc: 85.98,  30 12 10 5 2 26 23.75\n",
      "101/810, fold 5, train acc: 82.52, val acc: 81.98,  30 12 10 5 2 39 31.21\n",
      "avg:  88.26\n",
      "102/810, fold 1, train acc: 66.84, val acc: 71.53,  30 12 10 5 3 0 16.35\n",
      "102/810, fold 2, train acc: 81.45, val acc: 82.9,  30 12 10 5 3 87 35.42\n",
      "102/810, fold 3, train acc: 78.08, val acc: 60.68,  30 12 10 5 3 55 32.29\n",
      "102/810, fold 4, train acc: 83.24, val acc: 84.81,  30 12 10 5 3 49 31.18\n",
      "102/810, fold 5, train acc: 82.67, val acc: 92.67,  30 12 10 5 3 37 23.16\n",
      "avg:  78.52\n",
      "103/810, fold 1, train acc: 82.36, val acc: 65.95,  30 12 20 1 2 65 25.43\n",
      "103/810, fold 2, train acc: 80.69, val acc: 79.91,  30 12 20 1 2 19 15.75\n",
      "103/810, fold 3, train acc: 84.24, val acc: 79.81,  30 12 20 1 2 66 27.5\n",
      "103/810, fold 4, train acc: 85.47, val acc: 74.3,  30 12 20 1 2 62 27.04\n",
      "103/810, fold 5, train acc: 82.85, val acc: 63.03,  30 12 20 1 2 30 21.11\n",
      "avg:  72.6\n",
      "104/810, fold 1, train acc: 84.07, val acc: 75.94,  30 12 20 1 3 59 30.25\n",
      "104/810, fold 2, train acc: 83.98, val acc: 82.76,  30 12 20 1 3 89 27.19\n",
      "104/810, fold 3, train acc: 81.82, val acc: 81.38,  30 12 20 1 3 6 14.73\n",
      "104/810, fold 4, train acc: 81.54, val acc: 87.42,  30 12 20 1 3 7 13.37\n",
      "104/810, fold 5, train acc: 79.08, val acc: 87.67,  30 12 20 1 3 1 11.8\n",
      "avg:  83.03\n",
      "105/810, fold 1, train acc: 81.43, val acc: 72.98,  30 12 20 20 2 25 19.19\n",
      "105/810, fold 2, train acc: 81.03, val acc: 84.32,  30 12 20 20 2 36 17.59\n",
      "105/810, fold 3, train acc: 84.35, val acc: 82.03,  30 12 20 20 2 68 29.22\n",
      "105/810, fold 4, train acc: 78.37, val acc: 61.28,  30 12 20 20 2 68 28.79\n",
      "105/810, fold 5, train acc: 82.06, val acc: 83.89,  30 12 20 20 2 8 12.72\n",
      "avg:  76.9\n",
      "106/810, fold 1, train acc: 82.0, val acc: 88.06,  30 12 20 20 3 5 12.62\n",
      "106/810, fold 2, train acc: 79.32, val acc: 87.15,  30 12 20 20 3 86 30.79\n",
      "106/810, fold 3, train acc: 82.29, val acc: 86.31,  30 12 20 20 3 11 13.58\n",
      "106/810, fold 4, train acc: 82.19, val acc: 86.0,  30 12 20 20 3 12 14.12\n",
      "106/810, fold 5, train acc: 80.58, val acc: 80.62,  30 12 20 20 3 5 13.38\n",
      "avg:  85.63\n"
     ]
    }
   ],
   "source": [
    "cuda_enabled = True\n",
    "#train with hyperparameter tuning on validation set\n",
    "it = 0\n",
    "for ss in [15, 30, 45]:\n",
    "    for feat in [8, 10, 12]:\n",
    "        for k in [5, 10, 20]:\n",
    "            for s in [1, k, k//2]:\n",
    "                for p in [2, 3]:\n",
    "                    avg = []\n",
    "                    it += 1\n",
    "                    for fold in ['1', '2', '3', '4', '5']:\n",
    "                        \n",
    "                        total_it = 3*3*3*3*2*5\n",
    "                        start = time.time()\n",
    "                        ep = 0\n",
    "                        all_data = load_data(chunk_s=300, slide_s=ss, fold=fold, norm='inside_chunking', load_dump=True)\n",
    "                        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label = all_data\n",
    "                        z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "                        if cuda_enabled==True:\n",
    "                            model = Net(feat, k, s, p, z).cuda()\n",
    "                        else:\n",
    "                            model = Net(feat, k, s, p, z)\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                        train_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_train)), torch.tensor(np.array(label_train)),torch.tensor(np.array(train_f)))\n",
    "                        trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "                        val_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_val)), torch.tensor(np.array(label_val)))\n",
    "                        valloader = torch.utils.data.DataLoader(val_dataset, shuffle=True, batch_size=64)\n",
    "                        \n",
    "                        max_train_acc, max_val_acc = 0, 0\n",
    "                        for epoch in range(100):\n",
    "                            \n",
    "                            loss, tr_accuracy, file_accuracy = train()\n",
    "                            loss, accuracy = eval_(valloader, 'val')\n",
    "                            \n",
    "                            if accuracy>max_val_acc: \n",
    "                                max_train_acc = tr_accuracy\n",
    "                                max_val_acc = accuracy \n",
    "                                ep = epoch\n",
    "                                torch.save(model.state_dict(), 'cnnf8k5s1_f{}_w10s_s1s_.pth'.format(fold))\n",
    "                            elif epoch-ep > 40: \n",
    "                                break\n",
    "                        avg.append(max_val_acc)\n",
    "                        print('{}/{}, fold {}, train acc: {}, val acc: {}, '.format(it, total_it, fold, round(max_train_acc, 2), round(max_val_acc, 2)), ss, feat, k, s, p, ep, round(time.time() - start, 2))\n",
    "                    print('avg: ', round(sum(avg)/5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20, 32!, 47, [till 91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cuda_enabled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a1cafd1f5e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inside_chunking'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcuda_enabled\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cuda_enabled' is not defined"
     ]
    }
   ],
   "source": [
    "avg = []\n",
    "ss = 30; k = 10; feat = 12; s = 10; p =2;\n",
    "#model results on test set\n",
    "for fold in ['1', '2', '3', '4', '5']:\n",
    "    start = time.time()\n",
    "    ep = 0\n",
    "    data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label = load_data(chunk_s=300, slide_s=ss, fold=fold, norm='inside_chunking')\n",
    "    z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "    if cuda_enabled==True:\n",
    "        model = Net(feat, k, s, p, z).cuda()\n",
    "    else:\n",
    "        model = Net(feat, k, s, p, z)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_train)), torch.tensor(np.array(label_train)),torch.tensor(np.array(train_f)))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_test)), torch.tensor(np.array(label_test)), torch.tensor(np.array(test_f)))\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=64)\n",
    "    max_train_acc, max_test_acc = 0, 0\n",
    "    for epoch in range(100):\n",
    "        loss, tr_accuracy, file_accuracy = train()\n",
    "        if accuracy>max_train_acc: max_train_acc = accuracy\n",
    "        if accuracy>max_test_acc: \n",
    "            max_train_acc = tr_accuracy\n",
    "            max_test_acc = accuracy \n",
    "            ep = epoch\n",
    "        elif epoch-ep > 40: \n",
    "            break\n",
    "    avg.append(max_test_acc)\n",
    "    print('fold {}, train acc: {}, test acc: {}, '.format(fold, round(max_train_acc, 2), round(max_test_acc, 2)), ss, feat, k, s, p, ep, time.time() - start)\n",
    "print('avg: ', round(sum(avg)/5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get majority voting(file wise) values\n",
    "fold = '1'\n",
    "ss = 30\n",
    "data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_test)), torch.tensor(np.array(label_test)), torch.tensor(np.array(test_f)))\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=128)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "feat, k, s, p = 12, 10, 10, 2\n",
    "z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "model = Net(feat, k, s, p, z).cuda()\n",
    "model.load_state_dict(torch.load('cnnf8k5s1_f{}_w10s_s1s_.pth'.format(fold)))\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "#plot kernels \n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         if name == 'cnn.conv.weight':\n",
    "#             data = param.data\n",
    "# #             data = data.permute(2, 1, 0)\n",
    "#             for i in range(12):\n",
    "#                 plt.scatter(data[i][0].detach().cpu().numpy())\n",
    "#                 plt.show()\n",
    "#         #         plt.plot(param.data.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# model.load_state_dict('cnnf8k5s1_f1_w10s_s0.5s_.pth')\n",
    "model.eval()\n",
    "def eval_():\n",
    "    file_acc_ = {}\n",
    "    file_acc = {}\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_ = 0\n",
    "    outs = []\n",
    "    labs = []\n",
    "    mv_count = [0]*11\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        \n",
    "        inputs, labels, f = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        outs.extend(top_class.view(top_class.shape[0]).cpu().detach().numpy().tolist())\n",
    "        labs.extend(labels.cpu().detach().numpy().tolist())\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        for j in range(len(f)):\n",
    "            file = f[j].item()\n",
    "            if file not in file_acc:\n",
    "                file_acc[file] = []\n",
    "            file_acc[file].append(top_class[j].cpu().detach().numpy())\n",
    "            if file not in file_acc_:\n",
    "                file_acc_[file] = [0, 0]\n",
    "            if comp[j] == True:\n",
    "                file_acc_[file][0]+=1\n",
    "                file_acc_[file][1]+=1\n",
    "            else:\n",
    "                file_acc_[file][1]+=1\n",
    "    for i in file_acc_.keys():\n",
    "        file_acc_[i] = [test_set[i-len(train_set)], file_acc_[i][0]/file_acc_[i][1]*100]\n",
    "    cd = [0]*11\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    for j in range(1, 11):\n",
    "        cl = []\n",
    "        for i in file_acc.keys():\n",
    "            l = len(file_acc[i])\n",
    "            if l>9:\n",
    "                seg = file_acc[i][:j*l//10]\n",
    "            else:\n",
    "                if j<=l:\n",
    "                    seg = file_acc[i][:j*l//l]\n",
    "                else:\n",
    "                    seg = file_acc[i]\n",
    "            seg_acc = sum(seg)/len(seg)\n",
    "            if seg_acc > 0.5: #more ones than zeros\n",
    "                cl.append(1)\n",
    "            else:\n",
    "                cl.append(0) #more zeros than ones\n",
    "        print(round(accuracy_score(f_l, cl), 4))\n",
    "    print('~~~~')\n",
    "    for j in range(1, 11):\n",
    "        cl = []\n",
    "        for i in file_acc.keys():\n",
    "            l = len(file_acc[i])\n",
    "            if l>9:\n",
    "                seg = file_acc[i][:j*l//10]\n",
    "            else:\n",
    "                if j<=l:\n",
    "                    seg = file_acc[i][:j*l//l]\n",
    "                else:\n",
    "                    seg = file_acc[i]\n",
    "            seg_acc = sum(seg)/len(seg)\n",
    "            if seg_acc > 0.5: #more ones than zeros\n",
    "                cl.append(1)\n",
    "            else:\n",
    "                cl.append(0) #more zeros than ones\n",
    "        \n",
    "        print(round(f1_score(f_l, cl), 4))\n",
    "        print('majority:',sklearn.metrics.confusion_matrix(f_l, cl))\n",
    "    print('chunkwise:',round(accuracy_score(labs, outs), 4))\n",
    "    print('chunkwise:',sklearn.metrics.confusion_matrix(labs, outs))\n",
    "    return loss_/len(testloader), correct/len(testloader)*100,  outs, labs, file_acc_\n",
    "a = eval_()\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random():\n",
    "    r = np.random.randint(0, len(data_train))\n",
    "    plt.plot(data_train[r])\n",
    "    print(label_train[r])\n",
    "get_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = '5'\n",
    "ss = 30\n",
    "import sklearn.svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_val(svm):\n",
    "        fold = '1'\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_val))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_val).ravel(), y_pred))\n",
    "        \n",
    "for gamma in ['scale', 'auto']:\n",
    "    for degree in [2, 3, 4]:\n",
    "        for coef in [0, 0.5, 1, 5]:\n",
    "            for C in [0.1, 0.5, 1, 2]:\n",
    "                for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "                    print(gamma, degree, coef, C, kernel)\n",
    "                    svm = sklearn.svm.SVC(kernel=kernel, gamma=gamma, coef0=coef, C=C, degree=degree)\n",
    "                    get_svm_results_on_val(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_val(svm, fold='1'):\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_val))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_val).ravel(), y_pred))\n",
    "for gamma in ['auto']:\n",
    "    for degree in [3, 4]:\n",
    "        for coef in [0, 0.5, 1, 5]:\n",
    "            for C in [0.1, 0.5, 1, 2]:\n",
    "                for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "                    print(gamma, degree, coef, C, kernel)\n",
    "                    svm = sklearn.svm.SVC(kernel=kernel, gamma=gamma, coef0=coef, C=C, degree=degree)\n",
    "                    get_svm_results_on_val(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_test(svm, fold='1'):\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_test))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_test).ravel(), y_pred))\n",
    "for fold in ['1', '2', '3', '4', '5']:\n",
    "        get_svm_results_on_test(svm, fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
