{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(folder_count, file, signal, chunk_size, slide_size=15, norm=None):\n",
    "        data = []\n",
    "        label = []\n",
    "        f_label = []\n",
    "        n_chunks = int((len(signal)-chunk_size)/slide_size) +  1\n",
    "        for i in range(n_chunks):\n",
    "            chunk = signal[int(i*slide_size) : int(chunk_size+(i*slide_size))]\n",
    "            if norm =='inside_chunking':\n",
    "                chunk = normalise_data(chunk)\n",
    "            data.append(chunk)\n",
    "            label.append([int(file)])\n",
    "            f_label.append(folder_count)\n",
    "        return data, label, f_label, n_chunks\n",
    "    \n",
    "def normalise_data(data, norm_type='min_max'):\n",
    "    if norm_type=='z':\n",
    "        m = np.mean(data) \n",
    "        sd = np.sqrt((sum(data-m)**2)/len(data))\n",
    "        data = (data-m)/sd   \n",
    "    elif norm_type=='min_max':\n",
    "        data = (data - min(data))/(max(data)-min(data))\n",
    "    return data\n",
    "\n",
    "def load_data(fold, chunk_s, slide_s, dump=False, old=False, norm=False, split_percent=0.9):\n",
    "        data_train, label_train = [], []\n",
    "        data_test, label_test = [], []\n",
    "        train_f, test_f = [], []\n",
    "        chunks_count, full_label = [], []\n",
    "        train_set, test_set = [], []\n",
    "        data_val, label_val = [], []\n",
    "        folder_count = 0\n",
    "        files = sorted(os.listdir('data'))\n",
    "        f_check = {0:[], 1:[], 'l0':[], 'l1':[]}\n",
    "        \n",
    "        for i in range(len(files)):\n",
    "            file_label_ = int(files[i][-5])\n",
    "            with h5py.File('data/'+files[i], 'r') as f:\n",
    "                    data = np.array(f['reconstructed_calcium'])\n",
    "            if file_label_ == 1:\n",
    "                f_check[1].append(files[i])\n",
    "                f_check['l1'].append(len(data))\n",
    "            elif file_label_ == 0:\n",
    "                f_check[0].append(files[i])\n",
    "                f_check['l0'].append(len(data))\n",
    "\n",
    "        temp0 = list(zip(f_check['l0'], f_check[0]))\n",
    "        temp0.sort(reverse=True)\n",
    "        f_check['l0'], f_check[0] = zip(*temp0)  \n",
    "        temp0 = list(zip(f_check['l1'], f_check[1]))\n",
    "        temp0.sort(reverse=True)\n",
    "        f_check['l1'], f_check[1] = zip(*temp0)\n",
    "        f1, f2, f3, f4, f5 = [], [], [], [], []\n",
    "        \n",
    "        for i in range(len(f_check['l1'])):\n",
    "            if i%5 == 0: f1.append(f_check[1][i])\n",
    "            elif i%5 == 1: f2.append(f_check[1][i])\n",
    "            elif i%5 == 2: f3.append(f_check[1][i])\n",
    "            elif i%5 == 3: f4.append(f_check[1][i])\n",
    "            elif i%5 == 4: f5.append(f_check[1][i])\n",
    "        for i in range(len(f_check[0])):\n",
    "            if i%5 == 0: f1.append(f_check[0][i])\n",
    "            elif i%5 == 1: f2.append(f_check[0][i])\n",
    "            elif i%5 == 2: f3.append(f_check[0][i])\n",
    "            elif i%5 == 3: f4.append(f_check[0][i])\n",
    "            elif i%5 == 4: f5.append(f_check[0][i])\n",
    "                \n",
    "        if fold == '1': train_set = f2 + f3 + f4 + f5; test_set = f1;\n",
    "        if fold == '2': train_set = f1 + f3 + f4 + f5; test_set = f2;\n",
    "        if fold == '3': train_set = f1 + f2 + f4 + f5; test_set = f3;\n",
    "        if fold == '4': train_set = f1 + f2 + f3 + f5; test_set = f4;\n",
    "        if fold == '5': train_set = f1 + f2 + f3 + f4; test_set = f5;\n",
    "\n",
    "        for file in train_set:\n",
    "            with h5py.File('data/'+file, 'r') as f:\n",
    "                data = np.array(f['reconstructed_calcium'])\n",
    "            if norm=='before_chunking': data = normalise_data(data);\n",
    "            data_, label, f_label, c_c = chunks(folder_count, file=file[-5], signal=data, \n",
    "                                                chunk_size=chunk_s, slide_size=slide_s, norm=norm)\n",
    "            data_train.extend(data_); label_train.extend(label); train_f.extend(f_label)\n",
    "            folder_count += 1\n",
    "            \n",
    "        for file in test_set:\n",
    "            with h5py.File('data/'+file, 'r') as f:\n",
    "                data = np.array(f['reconstructed_calcium'])\n",
    "            if norm=='before_chunking': data = normalise_data(data);\n",
    "            data_, label, f_label, c_c = chunks(folder_count, file=file[-5], signal=data, \n",
    "                                                chunk_size=chunk_s, slide_size=slide_s, norm=norm)\n",
    "            data_test.extend(data_); chunks_count.append(c_c); label_test.extend(label)\n",
    "            test_f.extend(f_label); full_label.append(int(file[-5]))\n",
    "            folder_count += 1\n",
    "\n",
    "        data_val = data_train[int(len(data_train)*split_percent):]\n",
    "        data_train = data_train[:int(len(data_train)*split_percent)]\n",
    "        label_val = label_train[int(len(label_train)*split_percent):]\n",
    "        label_train = label_train[:int(len(label_train)*split_percent)]\n",
    "        train_f = train_f[:int(len(train_f)*split_percent)]\n",
    "        return data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feat, k, s, p, z):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, feat, kernel_size=k,stride=s)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(p)\n",
    "        self.feat = feat\n",
    "        self.z = z\n",
    "        self.linear = nn.Linear(feat*z,2)\n",
    "    def forward(self, x):\n",
    "        c_in = x.permute(0, 2, 1)\n",
    "        out = self.conv(c_in)\n",
    "        out = self.relu(out)\n",
    "        c_out = self.pool(out)\n",
    "        c_out = c_out.reshape(c_out.shape[0], self.feat*self.z)\n",
    "        c_out = self.linear(c_out)\n",
    "        return c_out\n",
    "\n",
    "def train():   \n",
    "    file_acc = {}\n",
    "    correct, loss_ = 0, 0\n",
    "    model.train()\n",
    "    outs, labs = [], []\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels, f = data\n",
    "        optimizer.zero_grad()\n",
    "        if cuda_enabled==True:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        else:\n",
    "            inputs, labels = inputs, labels.squeeze()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        for j in range(len(f)):\n",
    "            file = f[j].item()\n",
    "            if file not in file_acc:\n",
    "                file_acc[file] = [0, 0]\n",
    "            if comp[j] == True:\n",
    "                file_acc[file][0]+=1\n",
    "                file_acc[file][1]+=1\n",
    "            else:\n",
    "                file_acc[file][1]+=1\n",
    "    for i in file_acc.keys():\n",
    "        file_acc[i] = [train_set[i], file_acc[i][0]/file_acc[i][1]*100]\n",
    "    return loss_/len(trainloader), correct/len(trainloader)*100, file_acc\n",
    "\n",
    "def eval_(loader, mode='test'):\n",
    "    file_acc = {}\n",
    "    correct, loss_ = 0, 0\n",
    "    model.eval()\n",
    "    outs, labs = [], []\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        if mode=='test':\n",
    "            inputs, labels, f = data\n",
    "        else:\n",
    "            inputs, labels = data\n",
    "        if cuda_enabled==True:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        else:\n",
    "            inputs, labels = inputs, labels.squeeze()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        if mode=='test':\n",
    "            for j in range(len(f)):\n",
    "                file = f[j].item()\n",
    "                if file not in file_acc:\n",
    "                    file_acc[file] = [0, 0]\n",
    "                if comp[j] == True:\n",
    "                    file_acc[file][0]+=1\n",
    "                    file_acc[file][1]+=1\n",
    "                else:\n",
    "                    file_acc[file][1]+=1\n",
    "    if mode=='test':\n",
    "        for i in file_acc.keys():\n",
    "            file_acc[i] = [test_set[i-len(train_set)], file_acc[i][0]/file_acc[i][1]*100]\n",
    "        return loss_/len(loader), correct/len(loader)*100, file_acc\n",
    "    else:\n",
    "        return loss_/len(loader), correct/len(loader)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda_enabled = True\n",
    "#train with hyperparameter tuning on validation set\n",
    "for ss in [30]:\n",
    "    for feat in [12]:\n",
    "        for k in [10]:\n",
    "            for s in [10]:\n",
    "                for p in [2]:\n",
    "                    avg = []\n",
    "                    for fold in ['1', '2', '3', '4', '5']:\n",
    "                        start = time.time()\n",
    "                        ep = 0\n",
    "                        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label = load_data(chunk_s=300, slide_s=ss, fold=fold, norm='inside_chunking')\n",
    "                        z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "                        if cuda_enabled==True:\n",
    "                            model = Net(feat, k, s, p, z).cuda()\n",
    "                        else:\n",
    "                            model = Net(feat, k, s, p, z)\n",
    "                        criterion = nn.CrossEntropyLoss()\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                        train_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_train)), torch.tensor(np.array(label_train)),torch.tensor(np.array(train_f)))\n",
    "                        trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "                        val_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_val)), torch.tensor(np.array(label_val)))\n",
    "                        valloader = torch.utils.data.DataLoader(val_dataset, shuffle=True, batch_size=64)\n",
    "                        \n",
    "                        max_train_acc, max_val_acc = 0, 0\n",
    "                        for epoch in range(100):\n",
    "                            \n",
    "                            loss, tr_accuracy, file_accuracy = train()\n",
    "                            loss, accuracy = eval_(valloader, 'val')\n",
    "                            \n",
    "                            if accuracy>max_val_acc: \n",
    "                                max_train_acc = tr_accuracy\n",
    "                                max_val_acc = accuracy \n",
    "                                ep = epoch\n",
    "                                torch.save(model.state_dict(), 'cnnf8k5s1_f{}_w10s_s1s_.pth'.format(fold))\n",
    "                            elif epoch-ep > 40: \n",
    "                                break\n",
    "                        avg.append(max_val_acc)\n",
    "                        print('fold {}, train acc: {}, val acc: {}, '.format(fold, round(max_train_acc, 2), round(max_val_acc, 2)), ss, feat, k, s, p, ep, time.time() - start)\n",
    "                    print('avg: ', round(sum(avg)/5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "ss = 30; k = 10; feat = 12; s = 10; p =2;\n",
    "#model results on test set\n",
    "for fold in ['1', '2', '3', '4', '5']:\n",
    "    start = time.time()\n",
    "    ep = 0\n",
    "    data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, full_label = load_data(chunk_s=300, slide_s=ss, fold=fold, norm='inside_chunking')\n",
    "    z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "    if cuda_enabled==True:\n",
    "        model = Net(feat, k, s, p, z).cuda()\n",
    "    else:\n",
    "        model = Net(feat, k, s, p, z)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_train)), torch.tensor(np.array(label_train)),torch.tensor(np.array(train_f)))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=64)\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_test)), torch.tensor(np.array(label_test)), torch.tensor(np.array(test_f)))\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=64)\n",
    "    max_train_acc, max_test_acc = 0, 0\n",
    "    for epoch in range(100):\n",
    "        loss, tr_accuracy, file_accuracy = train()\n",
    "        if accuracy>max_train_acc: max_train_acc = accuracy\n",
    "        if accuracy>max_test_acc: \n",
    "            max_train_acc = tr_accuracy\n",
    "            max_test_acc = accuracy \n",
    "            ep = epoch\n",
    "        elif epoch-ep > 40: \n",
    "            break\n",
    "    avg.append(max_test_acc)\n",
    "    print('fold {}, train acc: {}, test acc: {}, '.format(fold, round(max_train_acc, 2), round(max_test_acc, 2)), ss, feat, k, s, p, ep, time.time() - start)\n",
    "print('avg: ', round(sum(avg)/5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get majority voting(file wise) values\n",
    "fold = '1'\n",
    "ss = 30\n",
    "data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(data_test)), torch.tensor(np.array(label_test)), torch.tensor(np.array(test_f)))\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=128)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "feat, k, s, p = 12, 10, 10, 2\n",
    "z = int(((len(data_train[0])-k)/s+1)//p)\n",
    "model = Net(feat, k, s, p, z).cuda()\n",
    "model.load_state_dict(torch.load('cnnf8k5s1_f{}_w10s_s1s_.pth'.format(fold)))\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "#plot kernels \n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         if name == 'cnn.conv.weight':\n",
    "#             data = param.data\n",
    "# #             data = data.permute(2, 1, 0)\n",
    "#             for i in range(12):\n",
    "#                 plt.scatter(data[i][0].detach().cpu().numpy())\n",
    "#                 plt.show()\n",
    "#         #         plt.plot(param.data.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# model.load_state_dict('cnnf8k5s1_f1_w10s_s0.5s_.pth')\n",
    "model.eval()\n",
    "def eval_():\n",
    "    file_acc_ = {}\n",
    "    file_acc = {}\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_ = 0\n",
    "    outs = []\n",
    "    labs = []\n",
    "    mv_count = [0]*11\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        \n",
    "        inputs, labels, f = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda().squeeze()\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss_ +=  loss.item()\n",
    "        softmax_output = torch.exp(outputs)\n",
    "        top_p, top_class = softmax_output.topk(1, dim=1)\n",
    "        outs.extend(top_class.view(top_class.shape[0]).cpu().detach().numpy().tolist())\n",
    "        labs.extend(labels.cpu().detach().numpy().tolist())\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        correct += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        comp = equals.squeeze().cpu().detach().numpy()\n",
    "        for j in range(len(f)):\n",
    "            file = f[j].item()\n",
    "            if file not in file_acc:\n",
    "                file_acc[file] = []\n",
    "            file_acc[file].append(top_class[j].cpu().detach().numpy())\n",
    "            if file not in file_acc_:\n",
    "                file_acc_[file] = [0, 0]\n",
    "            if comp[j] == True:\n",
    "                file_acc_[file][0]+=1\n",
    "                file_acc_[file][1]+=1\n",
    "            else:\n",
    "                file_acc_[file][1]+=1\n",
    "    for i in file_acc_.keys():\n",
    "        file_acc_[i] = [test_set[i-len(train_set)], file_acc_[i][0]/file_acc_[i][1]*100]\n",
    "    cd = [0]*11\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    for j in range(1, 11):\n",
    "        cl = []\n",
    "        for i in file_acc.keys():\n",
    "            l = len(file_acc[i])\n",
    "            if l>9:\n",
    "                seg = file_acc[i][:j*l//10]\n",
    "            else:\n",
    "                if j<=l:\n",
    "                    seg = file_acc[i][:j*l//l]\n",
    "                else:\n",
    "                    seg = file_acc[i]\n",
    "            seg_acc = sum(seg)/len(seg)\n",
    "            if seg_acc > 0.5: #more ones than zeros\n",
    "                cl.append(1)\n",
    "            else:\n",
    "                cl.append(0) #more zeros than ones\n",
    "        print(round(accuracy_score(f_l, cl), 4))\n",
    "    print('~~~~')\n",
    "    for j in range(1, 11):\n",
    "        cl = []\n",
    "        for i in file_acc.keys():\n",
    "            l = len(file_acc[i])\n",
    "            if l>9:\n",
    "                seg = file_acc[i][:j*l//10]\n",
    "            else:\n",
    "                if j<=l:\n",
    "                    seg = file_acc[i][:j*l//l]\n",
    "                else:\n",
    "                    seg = file_acc[i]\n",
    "            seg_acc = sum(seg)/len(seg)\n",
    "            if seg_acc > 0.5: #more ones than zeros\n",
    "                cl.append(1)\n",
    "            else:\n",
    "                cl.append(0) #more zeros than ones\n",
    "        \n",
    "        print(round(f1_score(f_l, cl), 4))\n",
    "        print('majority:',sklearn.metrics.confusion_matrix(f_l, cl))\n",
    "    print('chunkwise:',round(accuracy_score(labs, outs), 4))\n",
    "    print('chunkwise:',sklearn.metrics.confusion_matrix(labs, outs))\n",
    "    return loss_/len(testloader), correct/len(testloader)*100,  outs, labs, file_acc_\n",
    "a = eval_()\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random():\n",
    "    r = np.random.randint(0, len(data_train))\n",
    "    plt.plot(data_train[r])\n",
    "    print(label_train[r])\n",
    "get_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = '5'\n",
    "ss = 30\n",
    "import sklearn.svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_val(svm):\n",
    "        fold = '1'\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_val))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_val).ravel(), y_pred))\n",
    "        \n",
    "for gamma in ['scale', 'auto']:\n",
    "    for degree in [2, 3, 4]:\n",
    "        for coef in [0, 0.5, 1, 5]:\n",
    "            for C in [0.1, 0.5, 1, 2]:\n",
    "                for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "                    print(gamma, degree, coef, C, kernel)\n",
    "                    svm = sklearn.svm.SVC(kernel=kernel, gamma=gamma, coef0=coef, C=C, degree=degree)\n",
    "                    get_svm_results_on_val(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_val(svm, fold='1'):\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_val))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_val).ravel(), y_pred))\n",
    "for gamma in ['auto']:\n",
    "    for degree in [3, 4]:\n",
    "        for coef in [0, 0.5, 1, 5]:\n",
    "            for C in [0.1, 0.5, 1, 2]:\n",
    "                for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "                    print(gamma, degree, coef, C, kernel)\n",
    "                    svm = sklearn.svm.SVC(kernel=kernel, gamma=gamma, coef0=coef, C=C, degree=degree)\n",
    "                    get_svm_results_on_val(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_results_on_test(svm, fold='1'):\n",
    "        data_train, label_train, data_test, label_test, data_val, label_val, train_f, test_f, train_set, test_set, chunks_count, f_l = load_data(chunk_s=300, slide_s=ss, fold=fold, norm=True)\n",
    "        svm.fit(np.squeeze(data_train), np.array(label_train).ravel())\n",
    "        y_pred = svm.predict(np.squeeze(data_test))\n",
    "        print(fold, metrics.accuracy_score(np.array(label_test).ravel(), y_pred))\n",
    "for fold in ['1', '2', '3', '4', '5']:\n",
    "        get_svm_results_on_test(svm, fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
